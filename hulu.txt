#!/usr/bin/env python3
"""
Hulu-Med-7B inference (3D MRI NIfTI) - SKIP-FREE / DEBUGGABLE version

核心修复：
1) min_new_tokens 防止 generation==input（0 new tokens）
2) decode 两次：use_think=False 失败则 use_think=True
3) JSON 兼容单引号 (ast.literal_eval) + 更鲁棒的label提取
4) parse 支持中文关键词（心梗/瘢痕/正常/无）
5) 每个 skip 都打印原因与输出片段，最后汇总 skip_reason 统计
"""

import argparse
import json
import re
import time
import logging
import traceback
import ast
from collections import defaultdict
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import numpy as np
from PIL import Image
import torch
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoProcessor

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

import sys
sys.path.insert(0, str(Path(__file__).parent.parent))

from utils.data_utils import load_jsonl, load_nifti, apply_clahe_uint8

MI_LABEL = "myocardial_infarction_or_scar"
NEG_LABEL = "normal_or_no_lesion"
_JSON_OBJ_RE = re.compile(r"\{[\s\S]*\}")


def get_input_path(sample: Dict, preferred_type: str) -> Optional[str]:
    for inp in sample.get("inputs", []) or []:
        if inp.get("type") == preferred_type:
            return inp.get("path")
    return None


def select_uniform_indices(n: int, k: int, trim_ratio: float = 0.05) -> List[int]:
    if n <= 0 or k <= 0:
        return []
    if n <= k:
        return list(range(n))
    lo = int(round(trim_ratio * (n - 1)))
    hi = int(round((1.0 - trim_ratio) * (n - 1)))
    if hi <= lo or (hi - lo + 1) < k:
        lo, hi = 0, n - 1
    xs = np.linspace(lo, hi, num=k)
    idxs = [int(round(x)) for x in xs]
    seen, out = set(), []
    for i in idxs:
        if 0 <= i < n and i not in seen:
            out.append(i)
            seen.add(i)
    return out


def select_center_contiguous_indices(n: int, k: int, trim_ratio: float = 0.05) -> List[int]:
    if n <= 0 or k <= 0:
        return []
    if n <= k:
        return list(range(n))
    lo = int(round(trim_ratio * (n - 1)))
    hi = int(round((1.0 - trim_ratio) * (n - 1)))
    if hi <= lo:
        lo, hi = 0, n - 1
    span = hi - lo + 1
    if span <= 0:
        return []
    if span <= k:
        return list(range(lo, hi + 1))
    mid = (lo + hi) // 2
    start = mid - (k // 2)
    start = max(lo, min(start, hi - k + 1))
    return list(range(start, start + k))


def volume_to_pil_images(
    vol: np.ndarray,
    *,
    num_slices: int,
    axis: str = "z",
    trim_ratio: float = 0.05,
    sampling: str = "center",  # uniform | center
    norm_mode: str = "percentile",  # percentile | foreground
    norm_scope: str = "slice",  # slice | volume
    center_crop: int = 0,
    clahe: bool = False,
) -> Tuple[List[Image.Image], List[int]]:
    if vol.ndim != 3:
        raise ValueError(f"Expected 3D volume, got shape={vol.shape}")

    if axis == "z":
        n = vol.shape[2]
        get_slice = lambda idx: vol[:, :, idx]
    elif axis == "y":
        n = vol.shape[1]
        get_slice = lambda idx: vol[:, idx, :]
    elif axis == "x":
        n = vol.shape[0]
        get_slice = lambda idx: vol[idx, :, :]
    else:
        raise ValueError(f"Unsupported axis: {axis}")

    indices = (
        select_center_contiguous_indices(n, num_slices, trim_ratio=trim_ratio)
        if sampling == "center"
        else select_uniform_indices(n, num_slices, trim_ratio=trim_ratio)
    )

    def compute_lo_hi(arr: np.ndarray) -> Tuple[float, float]:
        arr = np.asarray(arr, dtype=np.float32)
        vals = arr[arr != 0] if norm_mode == "foreground" else arr.reshape(-1)
        if vals.size == 0:
            vals = arr.reshape(-1)
        lo = float(np.percentile(vals, 1))
        hi = float(np.percentile(vals, 99))
        if hi <= lo:
            hi = lo + 1.0
        return lo, hi

    vol_lo = vol_hi = None
    if norm_scope == "volume":
        vol_lo, vol_hi = compute_lo_hi(vol)

    images: List[Image.Image] = []
    for idx in indices:
        sl = get_slice(idx)
        if sl.ndim == 3:
            sl = sl[..., 0]

        lo, hi = (vol_lo, vol_hi) if (norm_scope == "volume" and vol_lo is not None) else compute_lo_hi(sl)
        sl = np.clip(sl, lo, hi)
        sl = (sl - lo) / (hi - lo + 1e-8)
        sl = np.clip(sl, 0.0, 1.0)
        if norm_mode == "foreground":
            sl = np.where(sl > 0, sl, 0.0)

        u8 = (sl * 255.0).astype(np.uint8)

        if center_crop and u8.ndim == 2:
            h, w = u8.shape
            size = min(int(center_crop), h, w)
            if size > 0:
                cx, cy = w // 2, h // 2
                x0 = max(0, cx - size // 2)
                y0 = max(0, cy - size // 2)
                u8 = u8[y0:y0 + size, x0:x0 + size]

        if clahe:
            try:
                u8 = apply_clahe_uint8(u8)
            except Exception:
                pass

        img = Image.fromarray(u8, mode="L").convert("RGB")
        images.append(img)

    return images, indices


def _resize_pil(img: Image.Image, size: int) -> Image.Image:
    if not size or size <= 0:
        return img
    try:
        return img.resize((size, size), Image.Resampling.BILINEAR)
    except AttributeError:
        return img.resize((size, size), Image.BILINEAR)


def build_prompt(num_images: int, style: str) -> str:
    # 强烈建议用 final_answer（最不容易 parse 失败）
    if style == "final_answer":
        return (
            f"Analyze this cardiac MRI volume with {num_images} slices. "
            f"Check specifically for {MI_LABEL}. "
            "Answer with EXACTLY one line: 'Final Answer: yes' or 'Final Answer: no'. "
            "('yes' means MI present; 'no' means normal/no lesion)"
        )
    return (
        f"Analyze this cardiac MRI volume with {num_images} slices. "
        f"Check specifically for {MI_LABEL}. "
        "You must output ONLY a valid JSON object, no other text. "
        f'Format: {{"label": "{MI_LABEL}" or "{NEG_LABEL}", "confidence": 0.0-1.0}}'
    )


def parse_model_output(text: str) -> Tuple[Optional[str], Optional[float], str]:
    raw = (text or "").strip()
    if not raw:
        return None, None, raw
    low = raw.lower()

    # 1) Final Answer
    m = re.search(r"final\s*answer\s*:\s*(yes|no)", low)
    if m:
        return (MI_LABEL if m.group(1) == "yes" else NEG_LABEL), None, raw

    # 2) 直接包含标签字符串
    if MI_LABEL in raw:
        return MI_LABEL, None, raw
    if NEG_LABEL in raw:
        return NEG_LABEL, None, raw

    # 3) JSON / dict 解析（支持单引号）
    m = _JSON_OBJ_RE.search(raw)
    blob = m.group(0) if m else None
    candidates = [blob] if blob else []
    candidates.append(raw)  # 最后整段也试一遍（有的人不输出大括号）

    for cand in candidates:
        if not cand:
            continue
        obj = None
        try:
            obj = json.loads(cand)
        except Exception:
            try:
                obj = ast.literal_eval(cand)  # 支持 {'label': '...'}
            except Exception:
                obj = None
        if isinstance(obj, dict):
            label = obj.get("label") or obj.get("pred") or obj.get("diagnosis")
            conf = obj.get("confidence")
            if isinstance(label, str):
                label = label.strip()
            conf = float(conf) if isinstance(conf, (int, float)) else None
            if label in ("yes", "Yes", "YES", True, "true", "True"):
                return MI_LABEL, conf, raw
            if label in ("no", "No", "NO", False, "false", "False"):
                return NEG_LABEL, conf, raw
            if label in (MI_LABEL, NEG_LABEL):
                return label, conf, raw

    # 4) yes/no 兜底
    if re.search(r"\byes\b", low):
        return MI_LABEL, None, raw
    if re.search(r"\bno\b", low):
        return NEG_LABEL, None, raw

    # 5) 中文关键词兜底
    if ("心梗" in raw) or ("心肌梗死" in raw) or ("瘢痕" in raw) or ("梗死" in raw):
        return MI_LABEL, None, raw
    if ("正常" in raw) or ("未见" in raw) or ("无明显" in raw) or ("未发现" in raw) or ("无梗死" in raw) or ("无瘢痕" in raw):
        return NEG_LABEL, None, raw

    return None, None, raw


def decode_best(processor, tokenizer, token_ids_1d: torch.Tensor) -> str:
    # 先去 think，再保留 think（避免“只有think没有final导致空串”）
    try:
        t = processor.decode(token_ids_1d.tolist(), skip_special_tokens=True, use_think=False).strip()
    except Exception:
        t = ""
    if not t:
        try:
            t = processor.decode(token_ids_1d.tolist(), skip_special_tokens=True, use_think=True).strip()
        except Exception:
            t = tokenizer.decode(token_ids_1d.tolist(), skip_special_tokens=True).strip()

    if "<|im_start|>assistant" in t:
        t = t.split("<|im_start|>assistant")[-1].strip()
    if "<|im_end|>" in t:
        t = t.replace("<|im_end|>", "").strip()
    return t


def main():
    parser = argparse.ArgumentParser(description="Hulu-Med-7B baseline inference (3D NIfTI) - debugged")
    parser.add_argument("--model_path", type=str, default="hf_models/Hulu-Med-7B")
    parser.add_argument("--index_jsonl", type=str, default="benchmark/data/index/index.jsonl")
    parser.add_argument("--datasets", type=str, default="ACDC,EMIDEC,MnMs2")
    parser.add_argument("--sample_ids", type=str, default="outputs/sample_ids_A_mi_binary_all_nifti_n200.json")
    parser.add_argument("--max_samples", type=int, default=1)

    parser.add_argument("--num_slices", type=int, default=8)
    parser.add_argument("--axis", type=str, default="z", choices=["x", "y", "z"])
    parser.add_argument("--trim_ratio", type=float, default=0.05)
    parser.add_argument("--no_canonical", action="store_true")
    parser.add_argument("--sampling", type=str, default="center", choices=["uniform", "center"])
    parser.add_argument("--norm_mode", type=str, default="percentile", choices=["percentile", "foreground"])
    parser.add_argument("--norm_scope", type=str, default="slice", choices=["slice", "volume"])
    parser.add_argument("--center_crop", type=int, default=0)
    parser.add_argument("--slice_markers", action="store_true")
    parser.add_argument("--clahe", action="store_true")

    parser.add_argument("--prompt_style", type=str, default="final_answer", choices=["json", "final_answer"])
    parser.add_argument("--max_new_tokens", type=int, default=256)
    parser.add_argument("--min_new_tokens", type=int, default=32)

    parser.add_argument("--output", type=str, default="outputs/predictions/hulumed_test_single.jsonl")
    parser.add_argument("--log_jsonl", type=str, default="outputs/logs/hulumed_test_single.jsonl")

    # 最稳：multi_image（完全绕开 video/3d RoPE 分支）
    parser.add_argument("--pack_3d", type=str, default="multi_image", choices=["multi_image", "native_3d"])
    parser.add_argument("--resize", type=int, default=280)

    args = parser.parse_args()

    model_path = Path(args.model_path)
    if not model_path.exists():
        raise FileNotFoundError(f"model_path not found: {model_path}")

    torch.backends.cuda.matmul.allow_tf32 = True
    torch.set_float32_matmul_precision("high")

    logger.info(f"Loading Hulu-Med from: {model_path}")
    model = AutoModelForCausalLM.from_pretrained(
        str(model_path),
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True,
        local_files_only=True,
    )
    processor = AutoProcessor.from_pretrained(
        str(model_path),
        trust_remote_code=True,
        local_files_only=True,
    )
    tokenizer = processor.tokenizer
    model.eval()

    all_samples = load_jsonl(args.index_jsonl)
    if args.sample_ids:
        target_ids = set(json.loads(Path(args.sample_ids).read_text(encoding="utf-8")))
        samples = [s for s in all_samples if s.get("sample_id") in target_ids]
    else:
        samples = all_samples

    if args.datasets:
        ds = set(x.strip() for x in args.datasets.split(",") if x.strip())
        samples = [s for s in samples if s.get("dataset") in ds]

    samples = [s for s in samples if s.get("task1")]
    if args.max_samples is not None:
        samples = samples[: args.max_samples]

    out_path = Path(args.output)
    out_path.parent.mkdir(parents=True, exist_ok=True)
    log_path = Path(args.log_jsonl)
    log_path.parent.mkdir(parents=True, exist_ok=True)
    out_path.write_text("", encoding="utf-8")

    logger.info(f"Found {len(samples)} samples. pack_3d={args.pack_3d}, prompt_style={args.prompt_style}")

    skipped = 0
    written = 0
    skip_stats = defaultdict(int)
    t0_all = time.time()

    with out_path.open("a", encoding="utf-8") as out_f:
        for sample in tqdm(samples, desc="Hulu-Med推理中"):
            sid = sample.get("sample_id")
            nifti_path = get_input_path(sample, "nifti")
            t0 = time.time()

            try:
                if not sid or not nifti_path or not Path(nifti_path).exists():
                    raise FileNotFoundError(f"missing_nifti: sid={sid}, path={nifti_path}")

                # 读取volume（multi_image必需；native_3d也先读一下便于排查）
                vol = load_nifti(nifti_path, canonical=(not args.no_canonical))
                if vol.ndim == 4:
                    vol = vol[..., 0]
                if vol.ndim != 3:
                    raise ValueError(f"Expected 3D volume after squeeze, got {vol.shape}")

                prompt = build_prompt(args.num_slices, args.prompt_style)

                axis_map = {"z": 2, "y": 1, "x": 0}
                nii_axis = axis_map.get(args.axis, 2)

                if args.pack_3d == "native_3d":
                    messages = [{
                        "role": "user",
                        "content": [
                            {"type": "3d", "3d": {"image_path": nifti_path, "nii_num_slices": args.num_slices, "nii_axis": nii_axis}},
                            {"type": "text", "text": prompt},
                        ],
                    }]
                else:
                    # multi_image：最稳
                    slice_imgs, slice_indices = volume_to_pil_images(
                        vol,
                        num_slices=args.num_slices,
                        axis=args.axis,
                        trim_ratio=args.trim_ratio,
                        sampling=args.sampling,
                        norm_mode=args.norm_mode,
                        norm_scope=args.norm_scope,
                        center_crop=args.center_crop,
                        clahe=args.clahe,
                    )
                    slice_imgs = [_resize_pil(im, args.resize) for im in slice_imgs]
                    content = []
                    for j, (im, idx) in enumerate(zip(slice_imgs, slice_indices)):
                        if args.slice_markers:
                            content.append({"type": "text", "text": f"Slice {j+1}/{len(slice_imgs)} (index={idx}):"})
                        content.append({"type": "image", "image": im})
                    content.append({"type": "text", "text": prompt})
                    messages = [{"role": "user", "content": content}]

                inputs = processor(
                    conversation=messages,
                    return_tensors="pt",
                    add_system_prompt=True,
                    add_generation_prompt=True,
                )

                # 关键：别静默，让你看到生成是否为0 token
                if "pixel_values" in inputs and isinstance(inputs["pixel_values"], torch.Tensor):
                    logger.info(f"[{sid}] pixel_values.shape={tuple(inputs['pixel_values'].shape)}")
                if "grid_sizes" in inputs and isinstance(inputs["grid_sizes"], torch.Tensor):
                    logger.info(f"[{sid}] grid_sizes={inputs['grid_sizes'].tolist()}")
                if "merge_sizes" in inputs and isinstance(inputs["merge_sizes"], torch.Tensor):
                    logger.info(f"[{sid}] merge_sizes={inputs['merge_sizes'].tolist()}")

                inputs = {k: v.to(model.device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}
                if "pixel_values" in inputs and isinstance(inputs["pixel_values"], torch.Tensor):
                    inputs["pixel_values"] = inputs["pixel_values"].to(torch.bfloat16)

                input_ids = inputs.get("input_ids")
                input_len = int(input_ids.shape[1]) if isinstance(input_ids, torch.Tensor) else 0

                with torch.inference_mode():
                    generation = model.generate(
                        **inputs,
                        max_new_tokens=max(args.max_new_tokens, 64),
                        min_new_tokens=max(args.min_new_tokens, 1),
                        do_sample=False,
                        pad_token_id=tokenizer.eos_token_id,
                    )

                gen_len = int(generation.shape[1])
                logger.info(f"[{sid}] input_len={input_len}, gen_len={gen_len}, new={max(0, gen_len - input_len)}")

                # 如果 new==0，直接把整段 decode（避免你trim后变空）
                if input_len > 0 and gen_len > input_len:
                    gen_trim = generation[0, input_len:]
                else:
                    gen_trim = generation[0]

                decoded = decode_best(processor, tokenizer, gen_trim)
                label, conf, _raw = parse_model_output(decoded)

                if label is None:
                    skipped += 1
                    skip_stats["parse_fail_or_empty"] += 1
                    logger.warning(f"[{sid}] SKIP parse_fail_or_empty. output[:200]={decoded[:200]!r}")
                    with log_path.open("a", encoding="utf-8") as lf:
                        lf.write(json.dumps({
                            "sample_id": sid,
                            "skip_reason": "parse_fail_or_empty",
                            "raw_output": decoded,
                            "nifti_path": nifti_path,
                            "pack_3d": args.pack_3d,
                            "input_len": input_len,
                            "gen_len": gen_len,
                            "elapsed_sec": time.time() - t0,
                        }, ensure_ascii=False) + "\n")
                    continue

                pred_row = {
                    "sample_id": sid,
                    "task": "task1",
                    "labels": [label],
                    "confidences": ({label: float(conf)} if conf is not None else {}),
                }
                out_f.write(json.dumps(pred_row, ensure_ascii=False) + "\n")
                out_f.flush()
                written += 1

                with log_path.open("a", encoding="utf-8") as lf:
                    lf.write(json.dumps({
                        "sample_id": sid,
                        "dataset": sample.get("dataset"),
                        "patient_id": sample.get("patient_id"),
                        "nifti_path": nifti_path,
                        "pack_3d": args.pack_3d,
                        "raw_output": decoded,
                        "final": pred_row,
                        "input_len": input_len,
                        "gen_len": gen_len,
                        "elapsed_sec": time.time() - t0,
                    }, ensure_ascii=False) + "\n")

            except Exception as e:
                skipped += 1
                skip_stats["exception"] += 1
                logger.error(f"[{sid}] EXCEPTION: {e}")
                logger.error(traceback.format_exc())
                with log_path.open("a", encoding="utf-8") as lf:
                    lf.write(json.dumps({
                        "sample_id": sid,
                        "skip_reason": "exception",
                        "error": str(e),
                        "traceback": traceback.format_exc(),
                        "nifti_path": nifti_path,
                        "pack_3d": args.pack_3d,
                        "elapsed_sec": time.time() - t0,
                    }, ensure_ascii=False) + "\n")
                continue

    logger.info(f"Done. written={written}, skipped={skipped}, elapsed_total_sec={time.time() - t0_all:.1f}")
    logger.info(f"Skip stats: {dict(skip_stats)}")


if __name__ == "__main__":
    main()
#!/usr/bin/env python3
"""
Hulu-Med-7B inference (3D MRI NIfTI) - SKIP-FREE / DEBUGGABLE version

核心修复：
1) min_new_tokens 防止 generation==input（0 new tokens）
2) decode 两次：use_think=False 失败则 use_think=True
3) JSON 兼容单引号 (ast.literal_eval) + 更鲁棒的label提取
4) parse 支持中文关键词（心梗/瘢痕/正常/无）
5) 每个 skip 都打印原因与输出片段，最后汇总 skip_reason 统计
"""

import argparse
import json
import re
import time
import logging
import traceback
import ast
from collections import defaultdict
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import numpy as np
from PIL import Image
import torch
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoProcessor

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

import sys
sys.path.insert(0, str(Path(__file__).parent.parent))

from utils.data_utils import load_jsonl, load_nifti, apply_clahe_uint8

MI_LABEL = "myocardial_infarction_or_scar"
NEG_LABEL = "normal_or_no_lesion"
_JSON_OBJ_RE = re.compile(r"\{[\s\S]*\}")


def get_input_path(sample: Dict, preferred_type: str) -> Optional[str]:
    for inp in sample.get("inputs", []) or []:
        if inp.get("type") == preferred_type:
            return inp.get("path")
    return None


def select_uniform_indices(n: int, k: int, trim_ratio: float = 0.05) -> List[int]:
    if n <= 0 or k <= 0:
        return []
    if n <= k:
        return list(range(n))
    lo = int(round(trim_ratio * (n - 1)))
    hi = int(round((1.0 - trim_ratio) * (n - 1)))
    if hi <= lo or (hi - lo + 1) < k:
        lo, hi = 0, n - 1
    xs = np.linspace(lo, hi, num=k)
    idxs = [int(round(x)) for x in xs]
    seen, out = set(), []
    for i in idxs:
        if 0 <= i < n and i not in seen:
            out.append(i)
            seen.add(i)
    return out


def select_center_contiguous_indices(n: int, k: int, trim_ratio: float = 0.05) -> List[int]:
    if n <= 0 or k <= 0:
        return []
    if n <= k:
        return list(range(n))
    lo = int(round(trim_ratio * (n - 1)))
    hi = int(round((1.0 - trim_ratio) * (n - 1)))
    if hi <= lo:
        lo, hi = 0, n - 1
    span = hi - lo + 1
    if span <= 0:
        return []
    if span <= k:
        return list(range(lo, hi + 1))
    mid = (lo + hi) // 2
    start = mid - (k // 2)
    start = max(lo, min(start, hi - k + 1))
    return list(range(start, start + k))


def volume_to_pil_images(
    vol: np.ndarray,
    *,
    num_slices: int,
    axis: str = "z",
    trim_ratio: float = 0.05,
    sampling: str = "center",  # uniform | center
    norm_mode: str = "percentile",  # percentile | foreground
    norm_scope: str = "slice",  # slice | volume
    center_crop: int = 0,
    clahe: bool = False,
) -> Tuple[List[Image.Image], List[int]]:
    if vol.ndim != 3:
        raise ValueError(f"Expected 3D volume, got shape={vol.shape}")

    if axis == "z":
        n = vol.shape[2]
        get_slice = lambda idx: vol[:, :, idx]
    elif axis == "y":
        n = vol.shape[1]
        get_slice = lambda idx: vol[:, idx, :]
    elif axis == "x":
        n = vol.shape[0]
        get_slice = lambda idx: vol[idx, :, :]
    else:
        raise ValueError(f"Unsupported axis: {axis}")

    indices = (
        select_center_contiguous_indices(n, num_slices, trim_ratio=trim_ratio)
        if sampling == "center"
        else select_uniform_indices(n, num_slices, trim_ratio=trim_ratio)
    )

    def compute_lo_hi(arr: np.ndarray) -> Tuple[float, float]:
        arr = np.asarray(arr, dtype=np.float32)
        vals = arr[arr != 0] if norm_mode == "foreground" else arr.reshape(-1)
        if vals.size == 0:
            vals = arr.reshape(-1)
        lo = float(np.percentile(vals, 1))
        hi = float(np.percentile(vals, 99))
        if hi <= lo:
            hi = lo + 1.0
        return lo, hi

    vol_lo = vol_hi = None
    if norm_scope == "volume":
        vol_lo, vol_hi = compute_lo_hi(vol)

    images: List[Image.Image] = []
    for idx in indices:
        sl = get_slice(idx)
        if sl.ndim == 3:
            sl = sl[..., 0]

        lo, hi = (vol_lo, vol_hi) if (norm_scope == "volume" and vol_lo is not None) else compute_lo_hi(sl)
        sl = np.clip(sl, lo, hi)
        sl = (sl - lo) / (hi - lo + 1e-8)
        sl = np.clip(sl, 0.0, 1.0)
        if norm_mode == "foreground":
            sl = np.where(sl > 0, sl, 0.0)

        u8 = (sl * 255.0).astype(np.uint8)

        if center_crop and u8.ndim == 2:
            h, w = u8.shape
            size = min(int(center_crop), h, w)
            if size > 0:
                cx, cy = w // 2, h // 2
                x0 = max(0, cx - size // 2)
                y0 = max(0, cy - size // 2)
                u8 = u8[y0:y0 + size, x0:x0 + size]

        if clahe:
            try:
                u8 = apply_clahe_uint8(u8)
            except Exception:
                pass

        img = Image.fromarray(u8, mode="L").convert("RGB")
        images.append(img)

    return images, indices


def _resize_pil(img: Image.Image, size: int) -> Image.Image:
    if not size or size <= 0:
        return img
    try:
        return img.resize((size, size), Image.Resampling.BILINEAR)
    except AttributeError:
        return img.resize((size, size), Image.BILINEAR)


def build_prompt(num_images: int, style: str) -> str:
    # 强烈建议用 final_answer（最不容易 parse 失败）
    if style == "final_answer":
        return (
            f"Analyze this cardiac MRI volume with {num_images} slices. "
            f"Check specifically for {MI_LABEL}. "
            "Answer with EXACTLY one line: 'Final Answer: yes' or 'Final Answer: no'. "
            "('yes' means MI present; 'no' means normal/no lesion)"
        )
    return (
        f"Analyze this cardiac MRI volume with {num_images} slices. "
        f"Check specifically for {MI_LABEL}. "
        "You must output ONLY a valid JSON object, no other text. "
        f'Format: {{"label": "{MI_LABEL}" or "{NEG_LABEL}", "confidence": 0.0-1.0}}'
    )


def parse_model_output(text: str) -> Tuple[Optional[str], Optional[float], str]:
    raw = (text or "").strip()
    if not raw:
        return None, None, raw
    low = raw.lower()

    # 1) Final Answer
    m = re.search(r"final\s*answer\s*:\s*(yes|no)", low)
    if m:
        return (MI_LABEL if m.group(1) == "yes" else NEG_LABEL), None, raw

    # 2) 直接包含标签字符串
    if MI_LABEL in raw:
        return MI_LABEL, None, raw
    if NEG_LABEL in raw:
        return NEG_LABEL, None, raw

    # 3) JSON / dict 解析（支持单引号）
    m = _JSON_OBJ_RE.search(raw)
    blob = m.group(0) if m else None
    candidates = [blob] if blob else []
    candidates.append(raw)  # 最后整段也试一遍（有的人不输出大括号）

    for cand in candidates:
        if not cand:
            continue
        obj = None
        try:
            obj = json.loads(cand)
        except Exception:
            try:
                obj = ast.literal_eval(cand)  # 支持 {'label': '...'}
            except Exception:
                obj = None
        if isinstance(obj, dict):
            label = obj.get("label") or obj.get("pred") or obj.get("diagnosis")
            conf = obj.get("confidence")
            if isinstance(label, str):
                label = label.strip()
            conf = float(conf) if isinstance(conf, (int, float)) else None
            if label in ("yes", "Yes", "YES", True, "true", "True"):
                return MI_LABEL, conf, raw
            if label in ("no", "No", "NO", False, "false", "False"):
                return NEG_LABEL, conf, raw
            if label in (MI_LABEL, NEG_LABEL):
                return label, conf, raw

    # 4) yes/no 兜底
    if re.search(r"\byes\b", low):
        return MI_LABEL, None, raw
    if re.search(r"\bno\b", low):
        return NEG_LABEL, None, raw

    # 5) 中文关键词兜底
    if ("心梗" in raw) or ("心肌梗死" in raw) or ("瘢痕" in raw) or ("梗死" in raw):
        return MI_LABEL, None, raw
    if ("正常" in raw) or ("未见" in raw) or ("无明显" in raw) or ("未发现" in raw) or ("无梗死" in raw) or ("无瘢痕" in raw):
        return NEG_LABEL, None, raw

    return None, None, raw


def decode_best(processor, tokenizer, token_ids_1d: torch.Tensor) -> str:
    # 先去 think，再保留 think（避免“只有think没有final导致空串”）
    try:
        t = processor.decode(token_ids_1d.tolist(), skip_special_tokens=True, use_think=False).strip()
    except Exception:
        t = ""
    if not t:
        try:
            t = processor.decode(token_ids_1d.tolist(), skip_special_tokens=True, use_think=True).strip()
        except Exception:
            t = tokenizer.decode(token_ids_1d.tolist(), skip_special_tokens=True).strip()

    if "<|im_start|>assistant" in t:
        t = t.split("<|im_start|>assistant")[-1].strip()
    if "<|im_end|>" in t:
        t = t.replace("<|im_end|>", "").strip()
    return t


def main():
    parser = argparse.ArgumentParser(description="Hulu-Med-7B baseline inference (3D NIfTI) - debugged")
    parser.add_argument("--model_path", type=str, default="hf_models/Hulu-Med-7B")
    parser.add_argument("--index_jsonl", type=str, default="benchmark/data/index/index.jsonl")
    parser.add_argument("--datasets", type=str, default="ACDC,EMIDEC,MnMs2")
    parser.add_argument("--sample_ids", type=str, default="outputs/sample_ids_A_mi_binary_all_nifti_n200.json")
    parser.add_argument("--max_samples", type=int, default=1)

    parser.add_argument("--num_slices", type=int, default=8)
    parser.add_argument("--axis", type=str, default="z", choices=["x", "y", "z"])
    parser.add_argument("--trim_ratio", type=float, default=0.05)
    parser.add_argument("--no_canonical", action="store_true")
    parser.add_argument("--sampling", type=str, default="center", choices=["uniform", "center"])
    parser.add_argument("--norm_mode", type=str, default="percentile", choices=["percentile", "foreground"])
    parser.add_argument("--norm_scope", type=str, default="slice", choices=["slice", "volume"])
    parser.add_argument("--center_crop", type=int, default=0)
    parser.add_argument("--slice_markers", action="store_true")
    parser.add_argument("--clahe", action="store_true")

    parser.add_argument("--prompt_style", type=str, default="final_answer", choices=["json", "final_answer"])
    parser.add_argument("--max_new_tokens", type=int, default=256)
    parser.add_argument("--min_new_tokens", type=int, default=32)

    parser.add_argument("--output", type=str, default="outputs/predictions/hulumed_test_single.jsonl")
    parser.add_argument("--log_jsonl", type=str, default="outputs/logs/hulumed_test_single.jsonl")

    # 最稳：multi_image（完全绕开 video/3d RoPE 分支）
    parser.add_argument("--pack_3d", type=str, default="multi_image", choices=["multi_image", "native_3d"])
    parser.add_argument("--resize", type=int, default=280)

    args = parser.parse_args()

    model_path = Path(args.model_path)
    if not model_path.exists():
        raise FileNotFoundError(f"model_path not found: {model_path}")

    torch.backends.cuda.matmul.allow_tf32 = True
    torch.set_float32_matmul_precision("high")

    logger.info(f"Loading Hulu-Med from: {model_path}")
    model = AutoModelForCausalLM.from_pretrained(
        str(model_path),
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True,
        local_files_only=True,
    )
    processor = AutoProcessor.from_pretrained(
        str(model_path),
        trust_remote_code=True,
        local_files_only=True,
    )
    tokenizer = processor.tokenizer
    model.eval()

    all_samples = load_jsonl(args.index_jsonl)
    if args.sample_ids:
        target_ids = set(json.loads(Path(args.sample_ids).read_text(encoding="utf-8")))
        samples = [s for s in all_samples if s.get("sample_id") in target_ids]
    else:
        samples = all_samples

    if args.datasets:
        ds = set(x.strip() for x in args.datasets.split(",") if x.strip())
        samples = [s for s in samples if s.get("dataset") in ds]

    samples = [s for s in samples if s.get("task1")]
    if args.max_samples is not None:
        samples = samples[: args.max_samples]

    out_path = Path(args.output)
    out_path.parent.mkdir(parents=True, exist_ok=True)
    log_path = Path(args.log_jsonl)
    log_path.parent.mkdir(parents=True, exist_ok=True)
    out_path.write_text("", encoding="utf-8")

    logger.info(f"Found {len(samples)} samples. pack_3d={args.pack_3d}, prompt_style={args.prompt_style}")

    skipped = 0
    written = 0
    skip_stats = defaultdict(int)
    t0_all = time.time()

    with out_path.open("a", encoding="utf-8") as out_f:
        for sample in tqdm(samples, desc="Hulu-Med推理中"):
            sid = sample.get("sample_id")
            nifti_path = get_input_path(sample, "nifti")
            t0 = time.time()

            try:
                if not sid or not nifti_path or not Path(nifti_path).exists():
                    raise FileNotFoundError(f"missing_nifti: sid={sid}, path={nifti_path}")

                # 读取volume（multi_image必需；native_3d也先读一下便于排查）
                vol = load_nifti(nifti_path, canonical=(not args.no_canonical))
                if vol.ndim == 4:
                    vol = vol[..., 0]
                if vol.ndim != 3:
                    raise ValueError(f"Expected 3D volume after squeeze, got {vol.shape}")

                prompt = build_prompt(args.num_slices, args.prompt_style)

                axis_map = {"z": 2, "y": 1, "x": 0}
                nii_axis = axis_map.get(args.axis, 2)

                if args.pack_3d == "native_3d":
                    messages = [{
                        "role": "user",
                        "content": [
                            {"type": "3d", "3d": {"image_path": nifti_path, "nii_num_slices": args.num_slices, "nii_axis": nii_axis}},
                            {"type": "text", "text": prompt},
                        ],
                    }]
                else:
                    # multi_image：最稳
                    slice_imgs, slice_indices = volume_to_pil_images(
                        vol,
                        num_slices=args.num_slices,
                        axis=args.axis,
                        trim_ratio=args.trim_ratio,
                        sampling=args.sampling,
                        norm_mode=args.norm_mode,
                        norm_scope=args.norm_scope,
                        center_crop=args.center_crop,
                        clahe=args.clahe,
                    )
                    slice_imgs = [_resize_pil(im, args.resize) for im in slice_imgs]
                    content = []
                    for j, (im, idx) in enumerate(zip(slice_imgs, slice_indices)):
                        if args.slice_markers:
                            content.append({"type": "text", "text": f"Slice {j+1}/{len(slice_imgs)} (index={idx}):"})
                        content.append({"type": "image", "image": im})
                    content.append({"type": "text", "text": prompt})
                    messages = [{"role": "user", "content": content}]

                inputs = processor(
                    conversation=messages,
                    return_tensors="pt",
                    add_system_prompt=True,
                    add_generation_prompt=True,
                )

                # 关键：别静默，让你看到生成是否为0 token
                if "pixel_values" in inputs and isinstance(inputs["pixel_values"], torch.Tensor):
                    logger.info(f"[{sid}] pixel_values.shape={tuple(inputs['pixel_values'].shape)}")
                if "grid_sizes" in inputs and isinstance(inputs["grid_sizes"], torch.Tensor):
                    logger.info(f"[{sid}] grid_sizes={inputs['grid_sizes'].tolist()}")
                if "merge_sizes" in inputs and isinstance(inputs["merge_sizes"], torch.Tensor):
                    logger.info(f"[{sid}] merge_sizes={inputs['merge_sizes'].tolist()}")

                inputs = {k: v.to(model.device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}
                if "pixel_values" in inputs and isinstance(inputs["pixel_values"], torch.Tensor):
                    inputs["pixel_values"] = inputs["pixel_values"].to(torch.bfloat16)

                input_ids = inputs.get("input_ids")
                input_len = int(input_ids.shape[1]) if isinstance(input_ids, torch.Tensor) else 0

                with torch.inference_mode():
                    generation = model.generate(
                        **inputs,
                        max_new_tokens=max(args.max_new_tokens, 64),
                        min_new_tokens=max(args.min_new_tokens, 1),
                        do_sample=False,
                        pad_token_id=tokenizer.eos_token_id,
                    )

                gen_len = int(generation.shape[1])
                logger.info(f"[{sid}] input_len={input_len}, gen_len={gen_len}, new={max(0, gen_len - input_len)}")

                # 如果 new==0，直接把整段 decode（避免你trim后变空）
                if input_len > 0 and gen_len > input_len:
                    gen_trim = generation[0, input_len:]
                else:
                    gen_trim = generation[0]

                decoded = decode_best(processor, tokenizer, gen_trim)
                label, conf, _raw = parse_model_output(decoded)

                if label is None:
                    skipped += 1
                    skip_stats["parse_fail_or_empty"] += 1
                    logger.warning(f"[{sid}] SKIP parse_fail_or_empty. output[:200]={decoded[:200]!r}")
                    with log_path.open("a", encoding="utf-8") as lf:
                        lf.write(json.dumps({
                            "sample_id": sid,
                            "skip_reason": "parse_fail_or_empty",
                            "raw_output": decoded,
                            "nifti_path": nifti_path,
                            "pack_3d": args.pack_3d,
                            "input_len": input_len,
                            "gen_len": gen_len,
                            "elapsed_sec": time.time() - t0,
                        }, ensure_ascii=False) + "\n")
                    continue

                pred_row = {
                    "sample_id": sid,
                    "task": "task1",
                    "labels": [label],
                    "confidences": ({label: float(conf)} if conf is not None else {}),
                }
                out_f.write(json.dumps(pred_row, ensure_ascii=False) + "\n")
                out_f.flush()
                written += 1

                with log_path.open("a", encoding="utf-8") as lf:
                    lf.write(json.dumps({
                        "sample_id": sid,
                        "dataset": sample.get("dataset"),
                        "patient_id": sample.get("patient_id"),
                        "nifti_path": nifti_path,
                        "pack_3d": args.pack_3d,
                        "raw_output": decoded,
                        "final": pred_row,
                        "input_len": input_len,
                        "gen_len": gen_len,
                        "elapsed_sec": time.time() - t0,
                    }, ensure_ascii=False) + "\n")

            except Exception as e:
                skipped += 1
                skip_stats["exception"] += 1
                logger.error(f"[{sid}] EXCEPTION: {e}")
                logger.error(traceback.format_exc())
                with log_path.open("a", encoding="utf-8") as lf:
                    lf.write(json.dumps({
                        "sample_id": sid,
                        "skip_reason": "exception",
                        "error": str(e),
                        "traceback": traceback.format_exc(),
                        "nifti_path": nifti_path,
                        "pack_3d": args.pack_3d,
                        "elapsed_sec": time.time() - t0,
                    }, ensure_ascii=False) + "\n")
                continue

    logger.info(f"Done. written={written}, skipped={skipped}, elapsed_total_sec={time.time() - t0_all:.1f}")
    logger.info(f"Skip stats: {dict(skip_stats)}")


if __name__ == "__main__":
    main()
#!/usr/bin/env python3
"""
Hulu-Med-7B inference (3D MRI NIfTI) - SKIP-FREE / DEBUGGABLE version

核心修复：
1) min_new_tokens 防止 generation==input（0 new tokens）
2) decode 两次：use_think=False 失败则 use_think=True
3) JSON 兼容单引号 (ast.literal_eval) + 更鲁棒的label提取
4) parse 支持中文关键词（心梗/瘢痕/正常/无）
5) 每个 skip 都打印原因与输出片段，最后汇总 skip_reason 统计
"""

import argparse
import json
import re
import time
import logging
import traceback
import ast
from collections import defaultdict
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import numpy as np
from PIL import Image
import torch
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoProcessor

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

import sys
sys.path.insert(0, str(Path(__file__).parent.parent))

from utils.data_utils import load_jsonl, load_nifti, apply_clahe_uint8

MI_LABEL = "myocardial_infarction_or_scar"
NEG_LABEL = "normal_or_no_lesion"
_JSON_OBJ_RE = re.compile(r"\{[\s\S]*\}")


def get_input_path(sample: Dict, preferred_type: str) -> Optional[str]:
    for inp in sample.get("inputs", []) or []:
        if inp.get("type") == preferred_type:
            return inp.get("path")
    return None


def select_uniform_indices(n: int, k: int, trim_ratio: float = 0.05) -> List[int]:
    if n <= 0 or k <= 0:
        return []
    if n <= k:
        return list(range(n))
    lo = int(round(trim_ratio * (n - 1)))
    hi = int(round((1.0 - trim_ratio) * (n - 1)))
    if hi <= lo or (hi - lo + 1) < k:
        lo, hi = 0, n - 1
    xs = np.linspace(lo, hi, num=k)
    idxs = [int(round(x)) for x in xs]
    seen, out = set(), []
    for i in idxs:
        if 0 <= i < n and i not in seen:
            out.append(i)
            seen.add(i)
    return out


def select_center_contiguous_indices(n: int, k: int, trim_ratio: float = 0.05) -> List[int]:
    if n <= 0 or k <= 0:
        return []
    if n <= k:
        return list(range(n))
    lo = int(round(trim_ratio * (n - 1)))
    hi = int(round((1.0 - trim_ratio) * (n - 1)))
    if hi <= lo:
        lo, hi = 0, n - 1
    span = hi - lo + 1
    if span <= 0:
        return []
    if span <= k:
        return list(range(lo, hi + 1))
    mid = (lo + hi) // 2
    start = mid - (k // 2)
    start = max(lo, min(start, hi - k + 1))
    return list(range(start, start + k))


def volume_to_pil_images(
    vol: np.ndarray,
    *,
    num_slices: int,
    axis: str = "z",
    trim_ratio: float = 0.05,
    sampling: str = "center",  # uniform | center
    norm_mode: str = "percentile",  # percentile | foreground
    norm_scope: str = "slice",  # slice | volume
    center_crop: int = 0,
    clahe: bool = False,
) -> Tuple[List[Image.Image], List[int]]:
    if vol.ndim != 3:
        raise ValueError(f"Expected 3D volume, got shape={vol.shape}")

    if axis == "z":
        n = vol.shape[2]
        get_slice = lambda idx: vol[:, :, idx]
    elif axis == "y":
        n = vol.shape[1]
        get_slice = lambda idx: vol[:, idx, :]
    elif axis == "x":
        n = vol.shape[0]
        get_slice = lambda idx: vol[idx, :, :]
    else:
        raise ValueError(f"Unsupported axis: {axis}")

    indices = (
        select_center_contiguous_indices(n, num_slices, trim_ratio=trim_ratio)
        if sampling == "center"
        else select_uniform_indices(n, num_slices, trim_ratio=trim_ratio)
    )

    def compute_lo_hi(arr: np.ndarray) -> Tuple[float, float]:
        arr = np.asarray(arr, dtype=np.float32)
        vals = arr[arr != 0] if norm_mode == "foreground" else arr.reshape(-1)
        if vals.size == 0:
            vals = arr.reshape(-1)
        lo = float(np.percentile(vals, 1))
        hi = float(np.percentile(vals, 99))
        if hi <= lo:
            hi = lo + 1.0
        return lo, hi

    vol_lo = vol_hi = None
    if norm_scope == "volume":
        vol_lo, vol_hi = compute_lo_hi(vol)

    images: List[Image.Image] = []
    for idx in indices:
        sl = get_slice(idx)
        if sl.ndim == 3:
            sl = sl[..., 0]

        lo, hi = (vol_lo, vol_hi) if (norm_scope == "volume" and vol_lo is not None) else compute_lo_hi(sl)
        sl = np.clip(sl, lo, hi)
        sl = (sl - lo) / (hi - lo + 1e-8)
        sl = np.clip(sl, 0.0, 1.0)
        if norm_mode == "foreground":
            sl = np.where(sl > 0, sl, 0.0)

        u8 = (sl * 255.0).astype(np.uint8)

        if center_crop and u8.ndim == 2:
            h, w = u8.shape
            size = min(int(center_crop), h, w)
            if size > 0:
                cx, cy = w // 2, h // 2
                x0 = max(0, cx - size // 2)
                y0 = max(0, cy - size // 2)
                u8 = u8[y0:y0 + size, x0:x0 + size]

        if clahe:
            try:
                u8 = apply_clahe_uint8(u8)
            except Exception:
                pass

        img = Image.fromarray(u8, mode="L").convert("RGB")
        images.append(img)

    return images, indices


def _resize_pil(img: Image.Image, size: int) -> Image.Image:
    if not size or size <= 0:
        return img
    try:
        return img.resize((size, size), Image.Resampling.BILINEAR)
    except AttributeError:
        return img.resize((size, size), Image.BILINEAR)


def build_prompt(num_images: int, style: str) -> str:
    # 强烈建议用 final_answer（最不容易 parse 失败）
    if style == "final_answer":
        return (
            f"Analyze this cardiac MRI volume with {num_images} slices. "
            f"Check specifically for {MI_LABEL}. "
            "Answer with EXACTLY one line: 'Final Answer: yes' or 'Final Answer: no'. "
            "('yes' means MI present; 'no' means normal/no lesion)"
        )
    return (
        f"Analyze this cardiac MRI volume with {num_images} slices. "
        f"Check specifically for {MI_LABEL}. "
        "You must output ONLY a valid JSON object, no other text. "
        f'Format: {{"label": "{MI_LABEL}" or "{NEG_LABEL}", "confidence": 0.0-1.0}}'
    )


def parse_model_output(text: str) -> Tuple[Optional[str], Optional[float], str]:
    raw = (text or "").strip()
    if not raw:
        return None, None, raw
    low = raw.lower()

    # 1) Final Answer
    m = re.search(r"final\s*answer\s*:\s*(yes|no)", low)
    if m:
        return (MI_LABEL if m.group(1) == "yes" else NEG_LABEL), None, raw

    # 2) 直接包含标签字符串
    if MI_LABEL in raw:
        return MI_LABEL, None, raw
    if NEG_LABEL in raw:
        return NEG_LABEL, None, raw

    # 3) JSON / dict 解析（支持单引号）
    m = _JSON_OBJ_RE.search(raw)
    blob = m.group(0) if m else None
    candidates = [blob] if blob else []
    candidates.append(raw)  # 最后整段也试一遍（有的人不输出大括号）

    for cand in candidates:
        if not cand:
            continue
        obj = None
        try:
            obj = json.loads(cand)
        except Exception:
            try:
                obj = ast.literal_eval(cand)  # 支持 {'label': '...'}
            except Exception:
                obj = None
        if isinstance(obj, dict):
            label = obj.get("label") or obj.get("pred") or obj.get("diagnosis")
            conf = obj.get("confidence")
            if isinstance(label, str):
                label = label.strip()
            conf = float(conf) if isinstance(conf, (int, float)) else None
            if label in ("yes", "Yes", "YES", True, "true", "True"):
                return MI_LABEL, conf, raw
            if label in ("no", "No", "NO", False, "false", "False"):
                return NEG_LABEL, conf, raw
            if label in (MI_LABEL, NEG_LABEL):
                return label, conf, raw

    # 4) yes/no 兜底
    if re.search(r"\byes\b", low):
        return MI_LABEL, None, raw
    if re.search(r"\bno\b", low):
        return NEG_LABEL, None, raw

    # 5) 中文关键词兜底
    if ("心梗" in raw) or ("心肌梗死" in raw) or ("瘢痕" in raw) or ("梗死" in raw):
        return MI_LABEL, None, raw
    if ("正常" in raw) or ("未见" in raw) or ("无明显" in raw) or ("未发现" in raw) or ("无梗死" in raw) or ("无瘢痕" in raw):
        return NEG_LABEL, None, raw

    return None, None, raw


def decode_best(processor, tokenizer, token_ids_1d: torch.Tensor) -> str:
    # 先去 think，再保留 think（避免“只有think没有final导致空串”）
    try:
        t = processor.decode(token_ids_1d.tolist(), skip_special_tokens=True, use_think=False).strip()
    except Exception:
        t = ""
    if not t:
        try:
            t = processor.decode(token_ids_1d.tolist(), skip_special_tokens=True, use_think=True).strip()
        except Exception:
            t = tokenizer.decode(token_ids_1d.tolist(), skip_special_tokens=True).strip()

    if "<|im_start|>assistant" in t:
        t = t.split("<|im_start|>assistant")[-1].strip()
    if "<|im_end|>" in t:
        t = t.replace("<|im_end|>", "").strip()
    return t


def main():
    parser = argparse.ArgumentParser(description="Hulu-Med-7B baseline inference (3D NIfTI) - debugged")
    parser.add_argument("--model_path", type=str, default="hf_models/Hulu-Med-7B")
    parser.add_argument("--index_jsonl", type=str, default="benchmark/data/index/index.jsonl")
    parser.add_argument("--datasets", type=str, default="ACDC,EMIDEC,MnMs2")
    parser.add_argument("--sample_ids", type=str, default="outputs/sample_ids_A_mi_binary_all_nifti_n200.json")
    parser.add_argument("--max_samples", type=int, default=1)

    parser.add_argument("--num_slices", type=int, default=8)
    parser.add_argument("--axis", type=str, default="z", choices=["x", "y", "z"])
    parser.add_argument("--trim_ratio", type=float, default=0.05)
    parser.add_argument("--no_canonical", action="store_true")
    parser.add_argument("--sampling", type=str, default="center", choices=["uniform", "center"])
    parser.add_argument("--norm_mode", type=str, default="percentile", choices=["percentile", "foreground"])
    parser.add_argument("--norm_scope", type=str, default="slice", choices=["slice", "volume"])
    parser.add_argument("--center_crop", type=int, default=0)
    parser.add_argument("--slice_markers", action="store_true")
    parser.add_argument("--clahe", action="store_true")

    parser.add_argument("--prompt_style", type=str, default="final_answer", choices=["json", "final_answer"])
    parser.add_argument("--max_new_tokens", type=int, default=256)
    parser.add_argument("--min_new_tokens", type=int, default=32)

    parser.add_argument("--output", type=str, default="outputs/predictions/hulumed_test_single.jsonl")
    parser.add_argument("--log_jsonl", type=str, default="outputs/logs/hulumed_test_single.jsonl")

    # 最稳：multi_image（完全绕开 video/3d RoPE 分支）
    parser.add_argument("--pack_3d", type=str, default="multi_image", choices=["multi_image", "native_3d"])
    parser.add_argument("--resize", type=int, default=280)

    args = parser.parse_args()

    model_path = Path(args.model_path)
    if not model_path.exists():
        raise FileNotFoundError(f"model_path not found: {model_path}")

    torch.backends.cuda.matmul.allow_tf32 = True
    torch.set_float32_matmul_precision("high")

    logger.info(f"Loading Hulu-Med from: {model_path}")
    model = AutoModelForCausalLM.from_pretrained(
        str(model_path),
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True,
        local_files_only=True,
    )
    processor = AutoProcessor.from_pretrained(
        str(model_path),
        trust_remote_code=True,
        local_files_only=True,
    )
    tokenizer = processor.tokenizer
    model.eval()

    all_samples = load_jsonl(args.index_jsonl)
    if args.sample_ids:
        target_ids = set(json.loads(Path(args.sample_ids).read_text(encoding="utf-8")))
        samples = [s for s in all_samples if s.get("sample_id") in target_ids]
    else:
        samples = all_samples

    if args.datasets:
        ds = set(x.strip() for x in args.datasets.split(",") if x.strip())
        samples = [s for s in samples if s.get("dataset") in ds]

    samples = [s for s in samples if s.get("task1")]
    if args.max_samples is not None:
        samples = samples[: args.max_samples]

    out_path = Path(args.output)
    out_path.parent.mkdir(parents=True, exist_ok=True)
    log_path = Path(args.log_jsonl)
    log_path.parent.mkdir(parents=True, exist_ok=True)
    out_path.write_text("", encoding="utf-8")

    logger.info(f"Found {len(samples)} samples. pack_3d={args.pack_3d}, prompt_style={args.prompt_style}")

    skipped = 0
    written = 0
    skip_stats = defaultdict(int)
    t0_all = time.time()

    with out_path.open("a", encoding="utf-8") as out_f:
        for sample in tqdm(samples, desc="Hulu-Med推理中"):
            sid = sample.get("sample_id")
            nifti_path = get_input_path(sample, "nifti")
            t0 = time.time()

            try:
                if not sid or not nifti_path or not Path(nifti_path).exists():
                    raise FileNotFoundError(f"missing_nifti: sid={sid}, path={nifti_path}")

                # 读取volume（multi_image必需；native_3d也先读一下便于排查）
                vol = load_nifti(nifti_path, canonical=(not args.no_canonical))
                if vol.ndim == 4:
                    vol = vol[..., 0]
                if vol.ndim != 3:
                    raise ValueError(f"Expected 3D volume after squeeze, got {vol.shape}")

                prompt = build_prompt(args.num_slices, args.prompt_style)

                axis_map = {"z": 2, "y": 1, "x": 0}
                nii_axis = axis_map.get(args.axis, 2)

                if args.pack_3d == "native_3d":
                    messages = [{
                        "role": "user",
                        "content": [
                            {"type": "3d", "3d": {"image_path": nifti_path, "nii_num_slices": args.num_slices, "nii_axis": nii_axis}},
                            {"type": "text", "text": prompt},
                        ],
                    }]
                else:
                    # multi_image：最稳
                    slice_imgs, slice_indices = volume_to_pil_images(
                        vol,
                        num_slices=args.num_slices,
                        axis=args.axis,
                        trim_ratio=args.trim_ratio,
                        sampling=args.sampling,
                        norm_mode=args.norm_mode,
                        norm_scope=args.norm_scope,
                        center_crop=args.center_crop,
                        clahe=args.clahe,
                    )
                    slice_imgs = [_resize_pil(im, args.resize) for im in slice_imgs]
                    content = []
                    for j, (im, idx) in enumerate(zip(slice_imgs, slice_indices)):
                        if args.slice_markers:
                            content.append({"type": "text", "text": f"Slice {j+1}/{len(slice_imgs)} (index={idx}):"})
                        content.append({"type": "image", "image": im})
                    content.append({"type": "text", "text": prompt})
                    messages = [{"role": "user", "content": content}]

                inputs = processor(
                    conversation=messages,
                    return_tensors="pt",
                    add_system_prompt=True,
                    add_generation_prompt=True,
                )

                # 关键：别静默，让你看到生成是否为0 token
                if "pixel_values" in inputs and isinstance(inputs["pixel_values"], torch.Tensor):
                    logger.info(f"[{sid}] pixel_values.shape={tuple(inputs['pixel_values'].shape)}")
                if "grid_sizes" in inputs and isinstance(inputs["grid_sizes"], torch.Tensor):
                    logger.info(f"[{sid}] grid_sizes={inputs['grid_sizes'].tolist()}")
                if "merge_sizes" in inputs and isinstance(inputs["merge_sizes"], torch.Tensor):
                    logger.info(f"[{sid}] merge_sizes={inputs['merge_sizes'].tolist()}")

                inputs = {k: v.to(model.device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}
                if "pixel_values" in inputs and isinstance(inputs["pixel_values"], torch.Tensor):
                    inputs["pixel_values"] = inputs["pixel_values"].to(torch.bfloat16)

                input_ids = inputs.get("input_ids")
                input_len = int(input_ids.shape[1]) if isinstance(input_ids, torch.Tensor) else 0

                with torch.inference_mode():
                    generation = model.generate(
                        **inputs,
                        max_new_tokens=max(args.max_new_tokens, 64),
                        min_new_tokens=max(args.min_new_tokens, 1),
                        do_sample=False,
                        pad_token_id=tokenizer.eos_token_id,
                    )

                gen_len = int(generation.shape[1])
                logger.info(f"[{sid}] input_len={input_len}, gen_len={gen_len}, new={max(0, gen_len - input_len)}")

                # 如果 new==0，直接把整段 decode（避免你trim后变空）
                if input_len > 0 and gen_len > input_len:
                    gen_trim = generation[0, input_len:]
                else:
                    gen_trim = generation[0]

                decoded = decode_best(processor, tokenizer, gen_trim)
                label, conf, _raw = parse_model_output(decoded)

                if label is None:
                    skipped += 1
                    skip_stats["parse_fail_or_empty"] += 1
                    logger.warning(f"[{sid}] SKIP parse_fail_or_empty. output[:200]={decoded[:200]!r}")
                    with log_path.open("a", encoding="utf-8") as lf:
                        lf.write(json.dumps({
                            "sample_id": sid,
                            "skip_reason": "parse_fail_or_empty",
                            "raw_output": decoded,
                            "nifti_path": nifti_path,
                            "pack_3d": args.pack_3d,
                            "input_len": input_len,
                            "gen_len": gen_len,
                            "elapsed_sec": time.time() - t0,
                        }, ensure_ascii=False) + "\n")
                    continue

                pred_row = {
                    "sample_id": sid,
                    "task": "task1",
                    "labels": [label],
                    "confidences": ({label: float(conf)} if conf is not None else {}),
                }
                out_f.write(json.dumps(pred_row, ensure_ascii=False) + "\n")
                out_f.flush()
                written += 1

                with log_path.open("a", encoding="utf-8") as lf:
                    lf.write(json.dumps({
                        "sample_id": sid,
                        "dataset": sample.get("dataset"),
                        "patient_id": sample.get("patient_id"),
                        "nifti_path": nifti_path,
                        "pack_3d": args.pack_3d,
                        "raw_output": decoded,
                        "final": pred_row,
                        "input_len": input_len,
                        "gen_len": gen_len,
                        "elapsed_sec": time.time() - t0,
                    }, ensure_ascii=False) + "\n")

            except Exception as e:
                skipped += 1
                skip_stats["exception"] += 1
                logger.error(f"[{sid}] EXCEPTION: {e}")
                logger.error(traceback.format_exc())
                with log_path.open("a", encoding="utf-8") as lf:
                    lf.write(json.dumps({
                        "sample_id": sid,
                        "skip_reason": "exception",
                        "error": str(e),
                        "traceback": traceback.format_exc(),
                        "nifti_path": nifti_path,
                        "pack_3d": args.pack_3d,
                        "elapsed_sec": time.time() - t0,
                    }, ensure_ascii=False) + "\n")
                continue

    logger.info(f"Done. written={written}, skipped={skipped}, elapsed_total_sec={time.time() - t0_all:.1f}")
    logger.info(f"Skip stats: {dict(skip_stats)}")


if __name__ == "__main__":
    main()
#!/usr/bin/env python3
"""
Hulu-Med-7B inference (3D MRI NIfTI) - SKIP-FREE / DEBUGGABLE version

核心修复：
1) min_new_tokens 防止 generation==input（0 new tokens）
2) decode 两次：use_think=False 失败则 use_think=True
3) JSON 兼容单引号 (ast.literal_eval) + 更鲁棒的label提取
4) parse 支持中文关键词（心梗/瘢痕/正常/无）
5) 每个 skip 都打印原因与输出片段，最后汇总 skip_reason 统计
"""

import argparse
import json
import re
import time
import logging
import traceback
import ast
from collections import defaultdict
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import numpy as np
from PIL import Image
import torch
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoProcessor

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

import sys
sys.path.insert(0, str(Path(__file__).parent.parent))

from utils.data_utils import load_jsonl, load_nifti, apply_clahe_uint8

MI_LABEL = "myocardial_infarction_or_scar"
NEG_LABEL = "normal_or_no_lesion"
_JSON_OBJ_RE = re.compile(r"\{[\s\S]*\}")


def get_input_path(sample: Dict, preferred_type: str) -> Optional[str]:
    for inp in sample.get("inputs", []) or []:
        if inp.get("type") == preferred_type:
            return inp.get("path")
    return None


def select_uniform_indices(n: int, k: int, trim_ratio: float = 0.05) -> List[int]:
    if n <= 0 or k <= 0:
        return []
    if n <= k:
        return list(range(n))
    lo = int(round(trim_ratio * (n - 1)))
    hi = int(round((1.0 - trim_ratio) * (n - 1)))
    if hi <= lo or (hi - lo + 1) < k:
        lo, hi = 0, n - 1
    xs = np.linspace(lo, hi, num=k)
    idxs = [int(round(x)) for x in xs]
    seen, out = set(), []
    for i in idxs:
        if 0 <= i < n and i not in seen:
            out.append(i)
            seen.add(i)
    return out


def select_center_contiguous_indices(n: int, k: int, trim_ratio: float = 0.05) -> List[int]:
    if n <= 0 or k <= 0:
        return []
    if n <= k:
        return list(range(n))
    lo = int(round(trim_ratio * (n - 1)))
    hi = int(round((1.0 - trim_ratio) * (n - 1)))
    if hi <= lo:
        lo, hi = 0, n - 1
    span = hi - lo + 1
    if span <= 0:
        return []
    if span <= k:
        return list(range(lo, hi + 1))
    mid = (lo + hi) // 2
    start = mid - (k // 2)
    start = max(lo, min(start, hi - k + 1))
    return list(range(start, start + k))


def volume_to_pil_images(
    vol: np.ndarray,
    *,
    num_slices: int,
    axis: str = "z",
    trim_ratio: float = 0.05,
    sampling: str = "center",  # uniform | center
    norm_mode: str = "percentile",  # percentile | foreground
    norm_scope: str = "slice",  # slice | volume
    center_crop: int = 0,
    clahe: bool = False,
) -> Tuple[List[Image.Image], List[int]]:
    if vol.ndim != 3:
        raise ValueError(f"Expected 3D volume, got shape={vol.shape}")

    if axis == "z":
        n = vol.shape[2]
        get_slice = lambda idx: vol[:, :, idx]
    elif axis == "y":
        n = vol.shape[1]
        get_slice = lambda idx: vol[:, idx, :]
    elif axis == "x":
        n = vol.shape[0]
        get_slice = lambda idx: vol[idx, :, :]
    else:
        raise ValueError(f"Unsupported axis: {axis}")

    indices = (
        select_center_contiguous_indices(n, num_slices, trim_ratio=trim_ratio)
        if sampling == "center"
        else select_uniform_indices(n, num_slices, trim_ratio=trim_ratio)
    )

    def compute_lo_hi(arr: np.ndarray) -> Tuple[float, float]:
        arr = np.asarray(arr, dtype=np.float32)
        vals = arr[arr != 0] if norm_mode == "foreground" else arr.reshape(-1)
        if vals.size == 0:
            vals = arr.reshape(-1)
        lo = float(np.percentile(vals, 1))
        hi = float(np.percentile(vals, 99))
        if hi <= lo:
            hi = lo + 1.0
        return lo, hi

    vol_lo = vol_hi = None
    if norm_scope == "volume":
        vol_lo, vol_hi = compute_lo_hi(vol)

    images: List[Image.Image] = []
    for idx in indices:
        sl = get_slice(idx)
        if sl.ndim == 3:
            sl = sl[..., 0]

        lo, hi = (vol_lo, vol_hi) if (norm_scope == "volume" and vol_lo is not None) else compute_lo_hi(sl)
        sl = np.clip(sl, lo, hi)
        sl = (sl - lo) / (hi - lo + 1e-8)
        sl = np.clip(sl, 0.0, 1.0)
        if norm_mode == "foreground":
            sl = np.where(sl > 0, sl, 0.0)

        u8 = (sl * 255.0).astype(np.uint8)

        if center_crop and u8.ndim == 2:
            h, w = u8.shape
            size = min(int(center_crop), h, w)
            if size > 0:
                cx, cy = w // 2, h // 2
                x0 = max(0, cx - size // 2)
                y0 = max(0, cy - size // 2)
                u8 = u8[y0:y0 + size, x0:x0 + size]

        if clahe:
            try:
                u8 = apply_clahe_uint8(u8)
            except Exception:
                pass

        img = Image.fromarray(u8, mode="L").convert("RGB")
        images.append(img)

    return images, indices


def _resize_pil(img: Image.Image, size: int) -> Image.Image:
    if not size or size <= 0:
        return img
    try:
        return img.resize((size, size), Image.Resampling.BILINEAR)
    except AttributeError:
        return img.resize((size, size), Image.BILINEAR)


def build_prompt(num_images: int, style: str) -> str:
    # 强烈建议用 final_answer（最不容易 parse 失败）
    if style == "final_answer":
        return (
            f"Analyze this cardiac MRI volume with {num_images} slices. "
            f"Check specifically for {MI_LABEL}. "
            "Answer with EXACTLY one line: 'Final Answer: yes' or 'Final Answer: no'. "
            "('yes' means MI present; 'no' means normal/no lesion)"
        )
    return (
        f"Analyze this cardiac MRI volume with {num_images} slices. "
        f"Check specifically for {MI_LABEL}. "
        "You must output ONLY a valid JSON object, no other text. "
        f'Format: {{"label": "{MI_LABEL}" or "{NEG_LABEL}", "confidence": 0.0-1.0}}'
    )


def parse_model_output(text: str) -> Tuple[Optional[str], Optional[float], str]:
    raw = (text or "").strip()
    if not raw:
        return None, None, raw
    low = raw.lower()

    # 1) Final Answer
    m = re.search(r"final\s*answer\s*:\s*(yes|no)", low)
    if m:
        return (MI_LABEL if m.group(1) == "yes" else NEG_LABEL), None, raw

    # 2) 直接包含标签字符串
    if MI_LABEL in raw:
        return MI_LABEL, None, raw
    if NEG_LABEL in raw:
        return NEG_LABEL, None, raw

    # 3) JSON / dict 解析（支持单引号）
    m = _JSON_OBJ_RE.search(raw)
    blob = m.group(0) if m else None
    candidates = [blob] if blob else []
    candidates.append(raw)  # 最后整段也试一遍（有的人不输出大括号）

    for cand in candidates:
        if not cand:
            continue
        obj = None
        try:
            obj = json.loads(cand)
        except Exception:
            try:
                obj = ast.literal_eval(cand)  # 支持 {'label': '...'}
            except Exception:
                obj = None
        if isinstance(obj, dict):
            label = obj.get("label") or obj.get("pred") or obj.get("diagnosis")
            conf = obj.get("confidence")
            if isinstance(label, str):
                label = label.strip()
            conf = float(conf) if isinstance(conf, (int, float)) else None
            if label in ("yes", "Yes", "YES", True, "true", "True"):
                return MI_LABEL, conf, raw
            if label in ("no", "No", "NO", False, "false", "False"):
                return NEG_LABEL, conf, raw
            if label in (MI_LABEL, NEG_LABEL):
                return label, conf, raw

    # 4) yes/no 兜底
    if re.search(r"\byes\b", low):
        return MI_LABEL, None, raw
    if re.search(r"\bno\b", low):
        return NEG_LABEL, None, raw

    # 5) 中文关键词兜底
    if ("心梗" in raw) or ("心肌梗死" in raw) or ("瘢痕" in raw) or ("梗死" in raw):
        return MI_LABEL, None, raw
    if ("正常" in raw) or ("未见" in raw) or ("无明显" in raw) or ("未发现" in raw) or ("无梗死" in raw) or ("无瘢痕" in raw):
        return NEG_LABEL, None, raw

    return None, None, raw


def decode_best(processor, tokenizer, token_ids_1d: torch.Tensor) -> str:
    # 先去 think，再保留 think（避免“只有think没有final导致空串”）
    try:
        t = processor.decode(token_ids_1d.tolist(), skip_special_tokens=True, use_think=False).strip()
    except Exception:
        t = ""
    if not t:
        try:
            t = processor.decode(token_ids_1d.tolist(), skip_special_tokens=True, use_think=True).strip()
        except Exception:
            t = tokenizer.decode(token_ids_1d.tolist(), skip_special_tokens=True).strip()

    if "<|im_start|>assistant" in t:
        t = t.split("<|im_start|>assistant")[-1].strip()
    if "<|im_end|>" in t:
        t = t.replace("<|im_end|>", "").strip()
    return t


def main():
    parser = argparse.ArgumentParser(description="Hulu-Med-7B baseline inference (3D NIfTI) - debugged")
    parser.add_argument("--model_path", type=str, default="hf_models/Hulu-Med-7B")
    parser.add_argument("--index_jsonl", type=str, default="benchmark/data/index/index.jsonl")
    parser.add_argument("--datasets", type=str, default="ACDC,EMIDEC,MnMs2")
    parser.add_argument("--sample_ids", type=str, default="outputs/sample_ids_A_mi_binary_all_nifti_n200.json")
    parser.add_argument("--max_samples", type=int, default=1)

    parser.add_argument("--num_slices", type=int, default=8)
    parser.add_argument("--axis", type=str, default="z", choices=["x", "y", "z"])
    parser.add_argument("--trim_ratio", type=float, default=0.05)
    parser.add_argument("--no_canonical", action="store_true")
    parser.add_argument("--sampling", type=str, default="center", choices=["uniform", "center"])
    parser.add_argument("--norm_mode", type=str, default="percentile", choices=["percentile", "foreground"])
    parser.add_argument("--norm_scope", type=str, default="slice", choices=["slice", "volume"])
    parser.add_argument("--center_crop", type=int, default=0)
    parser.add_argument("--slice_markers", action="store_true")
    parser.add_argument("--clahe", action="store_true")

    parser.add_argument("--prompt_style", type=str, default="final_answer", choices=["json", "final_answer"])
    parser.add_argument("--max_new_tokens", type=int, default=256)
    parser.add_argument("--min_new_tokens", type=int, default=32)

    parser.add_argument("--output", type=str, default="outputs/predictions/hulumed_test_single.jsonl")
    parser.add_argument("--log_jsonl", type=str, default="outputs/logs/hulumed_test_single.jsonl")

    # 最稳：multi_image（完全绕开 video/3d RoPE 分支）
    parser.add_argument("--pack_3d", type=str, default="multi_image", choices=["multi_image", "native_3d"])
    parser.add_argument("--resize", type=int, default=280)

    args = parser.parse_args()

    model_path = Path(args.model_path)
    if not model_path.exists():
        raise FileNotFoundError(f"model_path not found: {model_path}")

    torch.backends.cuda.matmul.allow_tf32 = True
    torch.set_float32_matmul_precision("high")

    logger.info(f"Loading Hulu-Med from: {model_path}")
    model = AutoModelForCausalLM.from_pretrained(
        str(model_path),
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True,
        local_files_only=True,
    )
    processor = AutoProcessor.from_pretrained(
        str(model_path),
        trust_remote_code=True,
        local_files_only=True,
    )
    tokenizer = processor.tokenizer
    model.eval()

    all_samples = load_jsonl(args.index_jsonl)
    if args.sample_ids:
        target_ids = set(json.loads(Path(args.sample_ids).read_text(encoding="utf-8")))
        samples = [s for s in all_samples if s.get("sample_id") in target_ids]
    else:
        samples = all_samples

    if args.datasets:
        ds = set(x.strip() for x in args.datasets.split(",") if x.strip())
        samples = [s for s in samples if s.get("dataset") in ds]

    samples = [s for s in samples if s.get("task1")]
    if args.max_samples is not None:
        samples = samples[: args.max_samples]

    out_path = Path(args.output)
    out_path.parent.mkdir(parents=True, exist_ok=True)
    log_path = Path(args.log_jsonl)
    log_path.parent.mkdir(parents=True, exist_ok=True)
    out_path.write_text("", encoding="utf-8")

    logger.info(f"Found {len(samples)} samples. pack_3d={args.pack_3d}, prompt_style={args.prompt_style}")

    skipped = 0
    written = 0
    skip_stats = defaultdict(int)
    t0_all = time.time()

    with out_path.open("a", encoding="utf-8") as out_f:
        for sample in tqdm(samples, desc="Hulu-Med推理中"):
            sid = sample.get("sample_id")
            nifti_path = get_input_path(sample, "nifti")
            t0 = time.time()

            try:
                if not sid or not nifti_path or not Path(nifti_path).exists():
                    raise FileNotFoundError(f"missing_nifti: sid={sid}, path={nifti_path}")

                # 读取volume（multi_image必需；native_3d也先读一下便于排查）
                vol = load_nifti(nifti_path, canonical=(not args.no_canonical))
                if vol.ndim == 4:
                    vol = vol[..., 0]
                if vol.ndim != 3:
                    raise ValueError(f"Expected 3D volume after squeeze, got {vol.shape}")

                prompt = build_prompt(args.num_slices, args.prompt_style)

                axis_map = {"z": 2, "y": 1, "x": 0}
                nii_axis = axis_map.get(args.axis, 2)

                if args.pack_3d == "native_3d":
                    messages = [{
                        "role": "user",
                        "content": [
                            {"type": "3d", "3d": {"image_path": nifti_path, "nii_num_slices": args.num_slices, "nii_axis": nii_axis}},
                            {"type": "text", "text": prompt},
                        ],
                    }]
                else:
                    # multi_image：最稳
                    slice_imgs, slice_indices = volume_to_pil_images(
                        vol,
                        num_slices=args.num_slices,
                        axis=args.axis,
                        trim_ratio=args.trim_ratio,
                        sampling=args.sampling,
                        norm_mode=args.norm_mode,
                        norm_scope=args.norm_scope,
                        center_crop=args.center_crop,
                        clahe=args.clahe,
                    )
                    slice_imgs = [_resize_pil(im, args.resize) for im in slice_imgs]
                    content = []
                    for j, (im, idx) in enumerate(zip(slice_imgs, slice_indices)):
                        if args.slice_markers:
                            content.append({"type": "text", "text": f"Slice {j+1}/{len(slice_imgs)} (index={idx}):"})
                        content.append({"type": "image", "image": im})
                    content.append({"type": "text", "text": prompt})
                    messages = [{"role": "user", "content": content}]

                inputs = processor(
                    conversation=messages,
                    return_tensors="pt",
                    add_system_prompt=True,
                    add_generation_prompt=True,
                )

                # 关键：别静默，让你看到生成是否为0 token
                if "pixel_values" in inputs and isinstance(inputs["pixel_values"], torch.Tensor):
                    logger.info(f"[{sid}] pixel_values.shape={tuple(inputs['pixel_values'].shape)}")
                if "grid_sizes" in inputs and isinstance(inputs["grid_sizes"], torch.Tensor):
                    logger.info(f"[{sid}] grid_sizes={inputs['grid_sizes'].tolist()}")
                if "merge_sizes" in inputs and isinstance(inputs["merge_sizes"], torch.Tensor):
                    logger.info(f"[{sid}] merge_sizes={inputs['merge_sizes'].tolist()}")

                inputs = {k: v.to(model.device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}
                if "pixel_values" in inputs and isinstance(inputs["pixel_values"], torch.Tensor):
                    inputs["pixel_values"] = inputs["pixel_values"].to(torch.bfloat16)

                input_ids = inputs.get("input_ids")
                input_len = int(input_ids.shape[1]) if isinstance(input_ids, torch.Tensor) else 0

                with torch.inference_mode():
                    generation = model.generate(
                        **inputs,
                        max_new_tokens=max(args.max_new_tokens, 64),
                        min_new_tokens=max(args.min_new_tokens, 1),
                        do_sample=False,
                        pad_token_id=tokenizer.eos_token_id,
                    )

                gen_len = int(generation.shape[1])
                logger.info(f"[{sid}] input_len={input_len}, gen_len={gen_len}, new={max(0, gen_len - input_len)}")

                # 如果 new==0，直接把整段 decode（避免你trim后变空）
                if input_len > 0 and gen_len > input_len:
                    gen_trim = generation[0, input_len:]
                else:
                    gen_trim = generation[0]

                decoded = decode_best(processor, tokenizer, gen_trim)
                label, conf, _raw = parse_model_output(decoded)

                if label is None:
                    skipped += 1
                    skip_stats["parse_fail_or_empty"] += 1
                    logger.warning(f"[{sid}] SKIP parse_fail_or_empty. output[:200]={decoded[:200]!r}")
                    with log_path.open("a", encoding="utf-8") as lf:
                        lf.write(json.dumps({
                            "sample_id": sid,
                            "skip_reason": "parse_fail_or_empty",
                            "raw_output": decoded,
                            "nifti_path": nifti_path,
                            "pack_3d": args.pack_3d,
                            "input_len": input_len,
                            "gen_len": gen_len,
                            "elapsed_sec": time.time() - t0,
                        }, ensure_ascii=False) + "\n")
                    continue

                pred_row = {
                    "sample_id": sid,
                    "task": "task1",
                    "labels": [label],
                    "confidences": ({label: float(conf)} if conf is not None else {}),
                }
                out_f.write(json.dumps(pred_row, ensure_ascii=False) + "\n")
                out_f.flush()
                written += 1

                with log_path.open("a", encoding="utf-8") as lf:
                    lf.write(json.dumps({
                        "sample_id": sid,
                        "dataset": sample.get("dataset"),
                        "patient_id": sample.get("patient_id"),
                        "nifti_path": nifti_path,
                        "pack_3d": args.pack_3d,
                        "raw_output": decoded,
                        "final": pred_row,
                        "input_len": input_len,
                        "gen_len": gen_len,
                        "elapsed_sec": time.time() - t0,
                    }, ensure_ascii=False) + "\n")

            except Exception as e:
                skipped += 1
                skip_stats["exception"] += 1
                logger.error(f"[{sid}] EXCEPTION: {e}")
                logger.error(traceback.format_exc())
                with log_path.open("a", encoding="utf-8") as lf:
                    lf.write(json.dumps({
                        "sample_id": sid,
                        "skip_reason": "exception",
                        "error": str(e),
                        "traceback": traceback.format_exc(),
                        "nifti_path": nifti_path,
                        "pack_3d": args.pack_3d,
                        "elapsed_sec": time.time() - t0,
                    }, ensure_ascii=False) + "\n")
                continue

    logger.info(f"Done. written={written}, skipped={skipped}, elapsed_total_sec={time.time() - t0_all:.1f}")
    logger.info(f"Skip stats: {dict(skip_stats)}")


if __name__ == "__main__":
    main()
#!/usr/bin/env python3
"""
Hulu-Med-7B inference (3D MRI NIfTI) - SKIP-FREE / DEBUGGABLE version

核心修复：
1) min_new_tokens 防止 generation==input（0 new tokens）
2) decode 两次：use_think=False 失败则 use_think=True
3) JSON 兼容单引号 (ast.literal_eval) + 更鲁棒的label提取
4) parse 支持中文关键词（心梗/瘢痕/正常/无）
5) 每个 skip 都打印原因与输出片段，最后汇总 skip_reason 统计
"""

import argparse
import json
import re
import time
import logging
import traceback
import ast
from collections import defaultdict
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import numpy as np
from PIL import Image
import torch
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoProcessor

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

import sys
sys.path.insert(0, str(Path(__file__).parent.parent))

from utils.data_utils import load_jsonl, load_nifti, apply_clahe_uint8

MI_LABEL = "myocardial_infarction_or_scar"
NEG_LABEL = "normal_or_no_lesion"
_JSON_OBJ_RE = re.compile(r"\{[\s\S]*\}")


def get_input_path(sample: Dict, preferred_type: str) -> Optional[str]:
    for inp in sample.get("inputs", []) or []:
        if inp.get("type") == preferred_type:
            return inp.get("path")
    return None


def select_uniform_indices(n: int, k: int, trim_ratio: float = 0.05) -> List[int]:
    if n <= 0 or k <= 0:
        return []
    if n <= k:
        return list(range(n))
    lo = int(round(trim_ratio * (n - 1)))
    hi = int(round((1.0 - trim_ratio) * (n - 1)))
    if hi <= lo or (hi - lo + 1) < k:
        lo, hi = 0, n - 1
    xs = np.linspace(lo, hi, num=k)
    idxs = [int(round(x)) for x in xs]
    seen, out = set(), []
    for i in idxs:
        if 0 <= i < n and i not in seen:
            out.append(i)
            seen.add(i)
    return out


def select_center_contiguous_indices(n: int, k: int, trim_ratio: float = 0.05) -> List[int]:
    if n <= 0 or k <= 0:
        return []
    if n <= k:
        return list(range(n))
    lo = int(round(trim_ratio * (n - 1)))
    hi = int(round((1.0 - trim_ratio) * (n - 1)))
    if hi <= lo:
        lo, hi = 0, n - 1
    span = hi - lo + 1
    if span <= 0:
        return []
    if span <= k:
        return list(range(lo, hi + 1))
    mid = (lo + hi) // 2
    start = mid - (k // 2)
    start = max(lo, min(start, hi - k + 1))
    return list(range(start, start + k))


def volume_to_pil_images(
    vol: np.ndarray,
    *,
    num_slices: int,
    axis: str = "z",
    trim_ratio: float = 0.05,
    sampling: str = "center",  # uniform | center
    norm_mode: str = "percentile",  # percentile | foreground
    norm_scope: str = "slice",  # slice | volume
    center_crop: int = 0,
    clahe: bool = False,
) -> Tuple[List[Image.Image], List[int]]:
    if vol.ndim != 3:
        raise ValueError(f"Expected 3D volume, got shape={vol.shape}")

    if axis == "z":
        n = vol.shape[2]
        get_slice = lambda idx: vol[:, :, idx]
    elif axis == "y":
        n = vol.shape[1]
        get_slice = lambda idx: vol[:, idx, :]
    elif axis == "x":
        n = vol.shape[0]
        get_slice = lambda idx: vol[idx, :, :]
    else:
        raise ValueError(f"Unsupported axis: {axis}")

    indices = (
        select_center_contiguous_indices(n, num_slices, trim_ratio=trim_ratio)
        if sampling == "center"
        else select_uniform_indices(n, num_slices, trim_ratio=trim_ratio)
    )

    def compute_lo_hi(arr: np.ndarray) -> Tuple[float, float]:
        arr = np.asarray(arr, dtype=np.float32)
        vals = arr[arr != 0] if norm_mode == "foreground" else arr.reshape(-1)
        if vals.size == 0:
            vals = arr.reshape(-1)
        lo = float(np.percentile(vals, 1))
        hi = float(np.percentile(vals, 99))
        if hi <= lo:
            hi = lo + 1.0
        return lo, hi

    vol_lo = vol_hi = None
    if norm_scope == "volume":
        vol_lo, vol_hi = compute_lo_hi(vol)

    images: List[Image.Image] = []
    for idx in indices:
        sl = get_slice(idx)
        if sl.ndim == 3:
            sl = sl[..., 0]

        lo, hi = (vol_lo, vol_hi) if (norm_scope == "volume" and vol_lo is not None) else compute_lo_hi(sl)
        sl = np.clip(sl, lo, hi)
        sl = (sl - lo) / (hi - lo + 1e-8)
        sl = np.clip(sl, 0.0, 1.0)
        if norm_mode == "foreground":
            sl = np.where(sl > 0, sl, 0.0)

        u8 = (sl * 255.0).astype(np.uint8)

        if center_crop and u8.ndim == 2:
            h, w = u8.shape
            size = min(int(center_crop), h, w)
            if size > 0:
                cx, cy = w // 2, h // 2
                x0 = max(0, cx - size // 2)
                y0 = max(0, cy - size // 2)
                u8 = u8[y0:y0 + size, x0:x0 + size]

        if clahe:
            try:
                u8 = apply_clahe_uint8(u8)
            except Exception:
                pass

        img = Image.fromarray(u8, mode="L").convert("RGB")
        images.append(img)

    return images, indices


def _resize_pil(img: Image.Image, size: int) -> Image.Image:
    if not size or size <= 0:
        return img
    try:
        return img.resize((size, size), Image.Resampling.BILINEAR)
    except AttributeError:
        return img.resize((size, size), Image.BILINEAR)


def build_prompt(num_images: int, style: str) -> str:
    # 强烈建议用 final_answer（最不容易 parse 失败）
    if style == "final_answer":
        return (
            f"Analyze this cardiac MRI volume with {num_images} slices. "
            f"Check specifically for {MI_LABEL}. "
            "Answer with EXACTLY one line: 'Final Answer: yes' or 'Final Answer: no'. "
            "('yes' means MI present; 'no' means normal/no lesion)"
        )
    return (
        f"Analyze this cardiac MRI volume with {num_images} slices. "
        f"Check specifically for {MI_LABEL}. "
        "You must output ONLY a valid JSON object, no other text. "
        f'Format: {{"label": "{MI_LABEL}" or "{NEG_LABEL}", "confidence": 0.0-1.0}}'
    )


def parse_model_output(text: str) -> Tuple[Optional[str], Optional[float], str]:
    raw = (text or "").strip()
    if not raw:
        return None, None, raw
    low = raw.lower()

    # 1) Final Answer
    m = re.search(r"final\s*answer\s*:\s*(yes|no)", low)
    if m:
        return (MI_LABEL if m.group(1) == "yes" else NEG_LABEL), None, raw

    # 2) 直接包含标签字符串
    if MI_LABEL in raw:
        return MI_LABEL, None, raw
    if NEG_LABEL in raw:
        return NEG_LABEL, None, raw

    # 3) JSON / dict 解析（支持单引号）
    m = _JSON_OBJ_RE.search(raw)
    blob = m.group(0) if m else None
    candidates = [blob] if blob else []
    candidates.append(raw)  # 最后整段也试一遍（有的人不输出大括号）

    for cand in candidates:
        if not cand:
            continue
        obj = None
        try:
            obj = json.loads(cand)
        except Exception:
            try:
                obj = ast.literal_eval(cand)  # 支持 {'label': '...'}
            except Exception:
                obj = None
        if isinstance(obj, dict):
            label = obj.get("label") or obj.get("pred") or obj.get("diagnosis")
            conf = obj.get("confidence")
            if isinstance(label, str):
                label = label.strip()
            conf = float(conf) if isinstance(conf, (int, float)) else None
            if label in ("yes", "Yes", "YES", True, "true", "True"):
                return MI_LABEL, conf, raw
            if label in ("no", "No", "NO", False, "false", "False"):
                return NEG_LABEL, conf, raw
            if label in (MI_LABEL, NEG_LABEL):
                return label, conf, raw

    # 4) yes/no 兜底
    if re.search(r"\byes\b", low):
        return MI_LABEL, None, raw
    if re.search(r"\bno\b", low):
        return NEG_LABEL, None, raw

    # 5) 中文关键词兜底
    if ("心梗" in raw) or ("心肌梗死" in raw) or ("瘢痕" in raw) or ("梗死" in raw):
        return MI_LABEL, None, raw
    if ("正常" in raw) or ("未见" in raw) or ("无明显" in raw) or ("未发现" in raw) or ("无梗死" in raw) or ("无瘢痕" in raw):
        return NEG_LABEL, None, raw

    return None, None, raw


def decode_best(processor, tokenizer, token_ids_1d: torch.Tensor) -> str:
    # 先去 think，再保留 think（避免“只有think没有final导致空串”）
    try:
        t = processor.decode(token_ids_1d.tolist(), skip_special_tokens=True, use_think=False).strip()
    except Exception:
        t = ""
    if not t:
        try:
            t = processor.decode(token_ids_1d.tolist(), skip_special_tokens=True, use_think=True).strip()
        except Exception:
            t = tokenizer.decode(token_ids_1d.tolist(), skip_special_tokens=True).strip()

    if "<|im_start|>assistant" in t:
        t = t.split("<|im_start|>assistant")[-1].strip()
    if "<|im_end|>" in t:
        t = t.replace("<|im_end|>", "").strip()
    return t


def main():
    parser = argparse.ArgumentParser(description="Hulu-Med-7B baseline inference (3D NIfTI) - debugged")
    parser.add_argument("--model_path", type=str, default="hf_models/Hulu-Med-7B")
    parser.add_argument("--index_jsonl", type=str, default="benchmark/data/index/index.jsonl")
    parser.add_argument("--datasets", type=str, default="ACDC,EMIDEC,MnMs2")
    parser.add_argument("--sample_ids", type=str, default="outputs/sample_ids_A_mi_binary_all_nifti_n200.json")
    parser.add_argument("--max_samples", type=int, default=1)

    parser.add_argument("--num_slices", type=int, default=8)
    parser.add_argument("--axis", type=str, default="z", choices=["x", "y", "z"])
    parser.add_argument("--trim_ratio", type=float, default=0.05)
    parser.add_argument("--no_canonical", action="store_true")
    parser.add_argument("--sampling", type=str, default="center", choices=["uniform", "center"])
    parser.add_argument("--norm_mode", type=str, default="percentile", choices=["percentile", "foreground"])
    parser.add_argument("--norm_scope", type=str, default="slice", choices=["slice", "volume"])
    parser.add_argument("--center_crop", type=int, default=0)
    parser.add_argument("--slice_markers", action="store_true")
    parser.add_argument("--clahe", action="store_true")

    parser.add_argument("--prompt_style", type=str, default="final_answer", choices=["json", "final_answer"])
    parser.add_argument("--max_new_tokens", type=int, default=256)
    parser.add_argument("--min_new_tokens", type=int, default=32)

    parser.add_argument("--output", type=str, default="outputs/predictions/hulumed_test_single.jsonl")
    parser.add_argument("--log_jsonl", type=str, default="outputs/logs/hulumed_test_single.jsonl")

    # 最稳：multi_image（完全绕开 video/3d RoPE 分支）
    parser.add_argument("--pack_3d", type=str, default="multi_image", choices=["multi_image", "native_3d"])
    parser.add_argument("--resize", type=int, default=280)

    args = parser.parse_args()

    model_path = Path(args.model_path)
    if not model_path.exists():
        raise FileNotFoundError(f"model_path not found: {model_path}")

    torch.backends.cuda.matmul.allow_tf32 = True
    torch.set_float32_matmul_precision("high")

    logger.info(f"Loading Hulu-Med from: {model_path}")
    model = AutoModelForCausalLM.from_pretrained(
        str(model_path),
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True,
        local_files_only=True,
    )
    processor = AutoProcessor.from_pretrained(
        str(model_path),
        trust_remote_code=True,
        local_files_only=True,
    )
    tokenizer = processor.tokenizer
    model.eval()

    all_samples = load_jsonl(args.index_jsonl)
    if args.sample_ids:
        target_ids = set(json.loads(Path(args.sample_ids).read_text(encoding="utf-8")))
        samples = [s for s in all_samples if s.get("sample_id") in target_ids]
    else:
        samples = all_samples

    if args.datasets:
        ds = set(x.strip() for x in args.datasets.split(",") if x.strip())
        samples = [s for s in samples if s.get("dataset") in ds]

    samples = [s for s in samples if s.get("task1")]
    if args.max_samples is not None:
        samples = samples[: args.max_samples]

    out_path = Path(args.output)
    out_path.parent.mkdir(parents=True, exist_ok=True)
    log_path = Path(args.log_jsonl)
    log_path.parent.mkdir(parents=True, exist_ok=True)
    out_path.write_text("", encoding="utf-8")

    logger.info(f"Found {len(samples)} samples. pack_3d={args.pack_3d}, prompt_style={args.prompt_style}")

    skipped = 0
    written = 0
    skip_stats = defaultdict(int)
    t0_all = time.time()

    with out_path.open("a", encoding="utf-8") as out_f:
        for sample in tqdm(samples, desc="Hulu-Med推理中"):
            sid = sample.get("sample_id")
            nifti_path = get_input_path(sample, "nifti")
            t0 = time.time()

            try:
                if not sid or not nifti_path or not Path(nifti_path).exists():
                    raise FileNotFoundError(f"missing_nifti: sid={sid}, path={nifti_path}")

                # 读取volume（multi_image必需；native_3d也先读一下便于排查）
                vol = load_nifti(nifti_path, canonical=(not args.no_canonical))
                if vol.ndim == 4:
                    vol = vol[..., 0]
                if vol.ndim != 3:
                    raise ValueError(f"Expected 3D volume after squeeze, got {vol.shape}")

                prompt = build_prompt(args.num_slices, args.prompt_style)

                axis_map = {"z": 2, "y": 1, "x": 0}
                nii_axis = axis_map.get(args.axis, 2)

                if args.pack_3d == "native_3d":
                    messages = [{
                        "role": "user",
                        "content": [
                            {"type": "3d", "3d": {"image_path": nifti_path, "nii_num_slices": args.num_slices, "nii_axis": nii_axis}},
                            {"type": "text", "text": prompt},
                        ],
                    }]
                else:
                    # multi_image：最稳
                    slice_imgs, slice_indices = volume_to_pil_images(
                        vol,
                        num_slices=args.num_slices,
                        axis=args.axis,
                        trim_ratio=args.trim_ratio,
                        sampling=args.sampling,
                        norm_mode=args.norm_mode,
                        norm_scope=args.norm_scope,
                        center_crop=args.center_crop,
                        clahe=args.clahe,
                    )
                    slice_imgs = [_resize_pil(im, args.resize) for im in slice_imgs]
                    content = []
                    for j, (im, idx) in enumerate(zip(slice_imgs, slice_indices)):
                        if args.slice_markers:
                            content.append({"type": "text", "text": f"Slice {j+1}/{len(slice_imgs)} (index={idx}):"})
                        content.append({"type": "image", "image": im})
                    content.append({"type": "text", "text": prompt})
                    messages = [{"role": "user", "content": content}]

                inputs = processor(
                    conversation=messages,
                    return_tensors="pt",
                    add_system_prompt=True,
                    add_generation_prompt=True,
                )

                # 关键：别静默，让你看到生成是否为0 token
                if "pixel_values" in inputs and isinstance(inputs["pixel_values"], torch.Tensor):
                    logger.info(f"[{sid}] pixel_values.shape={tuple(inputs['pixel_values'].shape)}")
                if "grid_sizes" in inputs and isinstance(inputs["grid_sizes"], torch.Tensor):
                    logger.info(f"[{sid}] grid_sizes={inputs['grid_sizes'].tolist()}")
                if "merge_sizes" in inputs and isinstance(inputs["merge_sizes"], torch.Tensor):
                    logger.info(f"[{sid}] merge_sizes={inputs['merge_sizes'].tolist()}")

                inputs = {k: v.to(model.device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}
                if "pixel_values" in inputs and isinstance(inputs["pixel_values"], torch.Tensor):
                    inputs["pixel_values"] = inputs["pixel_values"].to(torch.bfloat16)

                input_ids = inputs.get("input_ids")
                input_len = int(input_ids.shape[1]) if isinstance(input_ids, torch.Tensor) else 0

                with torch.inference_mode():
                    generation = model.generate(
                        **inputs,
                        max_new_tokens=max(args.max_new_tokens, 64),
                        min_new_tokens=max(args.min_new_tokens, 1),
                        do_sample=False,
                        pad_token_id=tokenizer.eos_token_id,
                    )

                gen_len = int(generation.shape[1])
                logger.info(f"[{sid}] input_len={input_len}, gen_len={gen_len}, new={max(0, gen_len - input_len)}")

                # 如果 new==0，直接把整段 decode（避免你trim后变空）
                if input_len > 0 and gen_len > input_len:
                    gen_trim = generation[0, input_len:]
                else:
                    gen_trim = generation[0]

                decoded = decode_best(processor, tokenizer, gen_trim)
                label, conf, _raw = parse_model_output(decoded)

                if label is None:
                    skipped += 1
                    skip_stats["parse_fail_or_empty"] += 1
                    logger.warning(f"[{sid}] SKIP parse_fail_or_empty. output[:200]={decoded[:200]!r}")
                    with log_path.open("a", encoding="utf-8") as lf:
                        lf.write(json.dumps({
                            "sample_id": sid,
                            "skip_reason": "parse_fail_or_empty",
                            "raw_output": decoded,
                            "nifti_path": nifti_path,
                            "pack_3d": args.pack_3d,
                            "input_len": input_len,
                            "gen_len": gen_len,
                            "elapsed_sec": time.time() - t0,
                        }, ensure_ascii=False) + "\n")
                    continue

                pred_row = {
                    "sample_id": sid,
                    "task": "task1",
                    "labels": [label],
                    "confidences": ({label: float(conf)} if conf is not None else {}),
                }
                out_f.write(json.dumps(pred_row, ensure_ascii=False) + "\n")
                out_f.flush()
                written += 1

                with log_path.open("a", encoding="utf-8") as lf:
                    lf.write(json.dumps({
                        "sample_id": sid,
                        "dataset": sample.get("dataset"),
                        "patient_id": sample.get("patient_id"),
                        "nifti_path": nifti_path,
                        "pack_3d": args.pack_3d,
                        "raw_output": decoded,
                        "final": pred_row,
                        "input_len": input_len,
                        "gen_len": gen_len,
                        "elapsed_sec": time.time() - t0,
                    }, ensure_ascii=False) + "\n")

            except Exception as e:
                skipped += 1
                skip_stats["exception"] += 1
                logger.error(f"[{sid}] EXCEPTION: {e}")
                logger.error(traceback.format_exc())
                with log_path.open("a", encoding="utf-8") as lf:
                    lf.write(json.dumps({
                        "sample_id": sid,
                        "skip_reason": "exception",
                        "error": str(e),
                        "traceback": traceback.format_exc(),
                        "nifti_path": nifti_path,
                        "pack_3d": args.pack_3d,
                        "elapsed_sec": time.time() - t0,
                    }, ensure_ascii=False) + "\n")
                continue

    logger.info(f"Done. written={written}, skipped={skipped}, elapsed_total_sec={time.time() - t0_all:.1f}")
    logger.info(f"Skip stats: {dict(skip_stats)}")


if __name__ == "__main__":
    main()
#!/usr/bin/env python3
"""
Hulu-Med-7B inference (3D MRI NIfTI) - SKIP-FREE / DEBUGGABLE version

核心修复：
1) min_new_tokens 防止 generation==input（0 new tokens）
2) decode 两次：use_think=False 失败则 use_think=True
3) JSON 兼容单引号 (ast.literal_eval) + 更鲁棒的label提取
4) parse 支持中文关键词（心梗/瘢痕/正常/无）
5) 每个 skip 都打印原因与输出片段，最后汇总 skip_reason 统计
"""

import argparse
import json
import re
import time
import logging
import traceback
import ast
from collections import defaultdict
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import numpy as np
from PIL import Image
import torch
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoProcessor

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

import sys
sys.path.insert(0, str(Path(__file__).parent.parent))

from utils.data_utils import load_jsonl, load_nifti, apply_clahe_uint8

MI_LABEL = "myocardial_infarction_or_scar"
NEG_LABEL = "normal_or_no_lesion"
_JSON_OBJ_RE = re.compile(r"\{[\s\S]*\}")


def get_input_path(sample: Dict, preferred_type: str) -> Optional[str]:
    for inp in sample.get("inputs", []) or []:
        if inp.get("type") == preferred_type:
            return inp.get("path")
    return None


def select_uniform_indices(n: int, k: int, trim_ratio: float = 0.05) -> List[int]:
    if n <= 0 or k <= 0:
        return []
    if n <= k:
        return list(range(n))
    lo = int(round(trim_ratio * (n - 1)))
    hi = int(round((1.0 - trim_ratio) * (n - 1)))
    if hi <= lo or (hi - lo + 1) < k:
        lo, hi = 0, n - 1
    xs = np.linspace(lo, hi, num=k)
    idxs = [int(round(x)) for x in xs]
    seen, out = set(), []
    for i in idxs:
        if 0 <= i < n and i not in seen:
            out.append(i)
            seen.add(i)
    return out


def select_center_contiguous_indices(n: int, k: int, trim_ratio: float = 0.05) -> List[int]:
    if n <= 0 or k <= 0:
        return []
    if n <= k:
        return list(range(n))
    lo = int(round(trim_ratio * (n - 1)))
    hi = int(round((1.0 - trim_ratio) * (n - 1)))
    if hi <= lo:
        lo, hi = 0, n - 1
    span = hi - lo + 1
    if span <= 0:
        return []
    if span <= k:
        return list(range(lo, hi + 1))
    mid = (lo + hi) // 2
    start = mid - (k // 2)
    start = max(lo, min(start, hi - k + 1))
    return list(range(start, start + k))


def volume_to_pil_images(
    vol: np.ndarray,
    *,
    num_slices: int,
    axis: str = "z",
    trim_ratio: float = 0.05,
    sampling: str = "center",  # uniform | center
    norm_mode: str = "percentile",  # percentile | foreground
    norm_scope: str = "slice",  # slice | volume
    center_crop: int = 0,
    clahe: bool = False,
) -> Tuple[List[Image.Image], List[int]]:
    if vol.ndim != 3:
        raise ValueError(f"Expected 3D volume, got shape={vol.shape}")

    if axis == "z":
        n = vol.shape[2]
        get_slice = lambda idx: vol[:, :, idx]
    elif axis == "y":
        n = vol.shape[1]
        get_slice = lambda idx: vol[:, idx, :]
    elif axis == "x":
        n = vol.shape[0]
        get_slice = lambda idx: vol[idx, :, :]
    else:
        raise ValueError(f"Unsupported axis: {axis}")

    indices = (
        select_center_contiguous_indices(n, num_slices, trim_ratio=trim_ratio)
        if sampling == "center"
        else select_uniform_indices(n, num_slices, trim_ratio=trim_ratio)
    )

    def compute_lo_hi(arr: np.ndarray) -> Tuple[float, float]:
        arr = np.asarray(arr, dtype=np.float32)
        vals = arr[arr != 0] if norm_mode == "foreground" else arr.reshape(-1)
        if vals.size == 0:
            vals = arr.reshape(-1)
        lo = float(np.percentile(vals, 1))
        hi = float(np.percentile(vals, 99))
        if hi <= lo:
            hi = lo + 1.0
        return lo, hi

    vol_lo = vol_hi = None
    if norm_scope == "volume":
        vol_lo, vol_hi = compute_lo_hi(vol)

    images: List[Image.Image] = []
    for idx in indices:
        sl = get_slice(idx)
        if sl.ndim == 3:
            sl = sl[..., 0]

        lo, hi = (vol_lo, vol_hi) if (norm_scope == "volume" and vol_lo is not None) else compute_lo_hi(sl)
        sl = np.clip(sl, lo, hi)
        sl = (sl - lo) / (hi - lo + 1e-8)
        sl = np.clip(sl, 0.0, 1.0)
        if norm_mode == "foreground":
            sl = np.where(sl > 0, sl, 0.0)

        u8 = (sl * 255.0).astype(np.uint8)

        if center_crop and u8.ndim == 2:
            h, w = u8.shape
            size = min(int(center_crop), h, w)
            if size > 0:
                cx, cy = w // 2, h // 2
                x0 = max(0, cx - size // 2)
                y0 = max(0, cy - size // 2)
                u8 = u8[y0:y0 + size, x0:x0 + size]

        if clahe:
            try:
                u8 = apply_clahe_uint8(u8)
            except Exception:
                pass

        img = Image.fromarray(u8, mode="L").convert("RGB")
        images.append(img)

    return images, indices


def _resize_pil(img: Image.Image, size: int) -> Image.Image:
    if not size or size <= 0:
        return img
    try:
        return img.resize((size, size), Image.Resampling.BILINEAR)
    except AttributeError:
        return img.resize((size, size), Image.BILINEAR)


def build_prompt(num_images: int, style: str) -> str:
    # 强烈建议用 final_answer（最不容易 parse 失败）
    if style == "final_answer":
        return (
            f"Analyze this cardiac MRI volume with {num_images} slices. "
            f"Check specifically for {MI_LABEL}. "
            "Answer with EXACTLY one line: 'Final Answer: yes' or 'Final Answer: no'. "
            "('yes' means MI present; 'no' means normal/no lesion)"
        )
    return (
        f"Analyze this cardiac MRI volume with {num_images} slices. "
        f"Check specifically for {MI_LABEL}. "
        "You must output ONLY a valid JSON object, no other text. "
        f'Format: {{"label": "{MI_LABEL}" or "{NEG_LABEL}", "confidence": 0.0-1.0}}'
    )


def parse_model_output(text: str) -> Tuple[Optional[str], Optional[float], str]:
    raw = (text or "").strip()
    if not raw:
        return None, None, raw
    low = raw.lower()

    # 1) Final Answer
    m = re.search(r"final\s*answer\s*:\s*(yes|no)", low)
    if m:
        return (MI_LABEL if m.group(1) == "yes" else NEG_LABEL), None, raw

    # 2) 直接包含标签字符串
    if MI_LABEL in raw:
        return MI_LABEL, None, raw
    if NEG_LABEL in raw:
        return NEG_LABEL, None, raw

    # 3) JSON / dict 解析（支持单引号）
    m = _JSON_OBJ_RE.search(raw)
    blob = m.group(0) if m else None
    candidates = [blob] if blob else []
    candidates.append(raw)  # 最后整段也试一遍（有的人不输出大括号）

    for cand in candidates:
        if not cand:
            continue
        obj = None
        try:
            obj = json.loads(cand)
        except Exception:
            try:
                obj = ast.literal_eval(cand)  # 支持 {'label': '...'}
            except Exception:
                obj = None
        if isinstance(obj, dict):
            label = obj.get("label") or obj.get("pred") or obj.get("diagnosis")
            conf = obj.get("confidence")
            if isinstance(label, str):
                label = label.strip()
            conf = float(conf) if isinstance(conf, (int, float)) else None
            if label in ("yes", "Yes", "YES", True, "true", "True"):
                return MI_LABEL, conf, raw
            if label in ("no", "No", "NO", False, "false", "False"):
                return NEG_LABEL, conf, raw
            if label in (MI_LABEL, NEG_LABEL):
                return label, conf, raw

    # 4) yes/no 兜底
    if re.search(r"\byes\b", low):
        return MI_LABEL, None, raw
    if re.search(r"\bno\b", low):
        return NEG_LABEL, None, raw

    # 5) 中文关键词兜底
    if ("心梗" in raw) or ("心肌梗死" in raw) or ("瘢痕" in raw) or ("梗死" in raw):
        return MI_LABEL, None, raw
    if ("正常" in raw) or ("未见" in raw) or ("无明显" in raw) or ("未发现" in raw) or ("无梗死" in raw) or ("无瘢痕" in raw):
        return NEG_LABEL, None, raw

    return None, None, raw


def decode_best(processor, tokenizer, token_ids_1d: torch.Tensor) -> str:
    # 先去 think，再保留 think（避免“只有think没有final导致空串”）
    try:
        t = processor.decode(token_ids_1d.tolist(), skip_special_tokens=True, use_think=False).strip()
    except Exception:
        t = ""
    if not t:
        try:
            t = processor.decode(token_ids_1d.tolist(), skip_special_tokens=True, use_think=True).strip()
        except Exception:
            t = tokenizer.decode(token_ids_1d.tolist(), skip_special_tokens=True).strip()

    if "<|im_start|>assistant" in t:
        t = t.split("<|im_start|>assistant")[-1].strip()
    if "<|im_end|>" in t:
        t = t.replace("<|im_end|>", "").strip()
    return t


def main():
    parser = argparse.ArgumentParser(description="Hulu-Med-7B baseline inference (3D NIfTI) - debugged")
    parser.add_argument("--model_path", type=str, default="hf_models/Hulu-Med-7B")
    parser.add_argument("--index_jsonl", type=str, default="benchmark/data/index/index.jsonl")
    parser.add_argument("--datasets", type=str, default="ACDC,EMIDEC,MnMs2")
    parser.add_argument("--sample_ids", type=str, default="outputs/sample_ids_A_mi_binary_all_nifti_n200.json")
    parser.add_argument("--max_samples", type=int, default=1)

    parser.add_argument("--num_slices", type=int, default=8)
    parser.add_argument("--axis", type=str, default="z", choices=["x", "y", "z"])
    parser.add_argument("--trim_ratio", type=float, default=0.05)
    parser.add_argument("--no_canonical", action="store_true")
    parser.add_argument("--sampling", type=str, default="center", choices=["uniform", "center"])
    parser.add_argument("--norm_mode", type=str, default="percentile", choices=["percentile", "foreground"])
    parser.add_argument("--norm_scope", type=str, default="slice", choices=["slice", "volume"])
    parser.add_argument("--center_crop", type=int, default=0)
    parser.add_argument("--slice_markers", action="store_true")
    parser.add_argument("--clahe", action="store_true")

    parser.add_argument("--prompt_style", type=str, default="final_answer", choices=["json", "final_answer"])
    parser.add_argument("--max_new_tokens", type=int, default=256)
    parser.add_argument("--min_new_tokens", type=int, default=32)

    parser.add_argument("--output", type=str, default="outputs/predictions/hulumed_test_single.jsonl")
    parser.add_argument("--log_jsonl", type=str, default="outputs/logs/hulumed_test_single.jsonl")

    # 最稳：multi_image（完全绕开 video/3d RoPE 分支）
    parser.add_argument("--pack_3d", type=str, default="multi_image", choices=["multi_image", "native_3d"])
    parser.add_argument("--resize", type=int, default=280)

    args = parser.parse_args()

    model_path = Path(args.model_path)
    if not model_path.exists():
        raise FileNotFoundError(f"model_path not found: {model_path}")

    torch.backends.cuda.matmul.allow_tf32 = True
    torch.set_float32_matmul_precision("high")

    logger.info(f"Loading Hulu-Med from: {model_path}")
    model = AutoModelForCausalLM.from_pretrained(
        str(model_path),
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True,
        local_files_only=True,
    )
    processor = AutoProcessor.from_pretrained(
        str(model_path),
        trust_remote_code=True,
        local_files_only=True,
    )
    tokenizer = processor.tokenizer
    model.eval()

    all_samples = load_jsonl(args.index_jsonl)
    if args.sample_ids:
        target_ids = set(json.loads(Path(args.sample_ids).read_text(encoding="utf-8")))
        samples = [s for s in all_samples if s.get("sample_id") in target_ids]
    else:
        samples = all_samples

    if args.datasets:
        ds = set(x.strip() for x in args.datasets.split(",") if x.strip())
        samples = [s for s in samples if s.get("dataset") in ds]

    samples = [s for s in samples if s.get("task1")]
    if args.max_samples is not None:
        samples = samples[: args.max_samples]

    out_path = Path(args.output)
    out_path.parent.mkdir(parents=True, exist_ok=True)
    log_path = Path(args.log_jsonl)
    log_path.parent.mkdir(parents=True, exist_ok=True)
    out_path.write_text("", encoding="utf-8")

    logger.info(f"Found {len(samples)} samples. pack_3d={args.pack_3d}, prompt_style={args.prompt_style}")

    skipped = 0
    written = 0
    skip_stats = defaultdict(int)
    t0_all = time.time()

    with out_path.open("a", encoding="utf-8") as out_f:
        for sample in tqdm(samples, desc="Hulu-Med推理中"):
            sid = sample.get("sample_id")
            nifti_path = get_input_path(sample, "nifti")
            t0 = time.time()

            try:
                if not sid or not nifti_path or not Path(nifti_path).exists():
                    raise FileNotFoundError(f"missing_nifti: sid={sid}, path={nifti_path}")

                # 读取volume（multi_image必需；native_3d也先读一下便于排查）
                vol = load_nifti(nifti_path, canonical=(not args.no_canonical))
                if vol.ndim == 4:
                    vol = vol[..., 0]
                if vol.ndim != 3:
                    raise ValueError(f"Expected 3D volume after squeeze, got {vol.shape}")

                prompt = build_prompt(args.num_slices, args.prompt_style)

                axis_map = {"z": 2, "y": 1, "x": 0}
                nii_axis = axis_map.get(args.axis, 2)

                if args.pack_3d == "native_3d":
                    messages = [{
                        "role": "user",
                        "content": [
                            {"type": "3d", "3d": {"image_path": nifti_path, "nii_num_slices": args.num_slices, "nii_axis": nii_axis}},
                            {"type": "text", "text": prompt},
                        ],
                    }]
                else:
                    # multi_image：最稳
                    slice_imgs, slice_indices = volume_to_pil_images(
                        vol,
                        num_slices=args.num_slices,
                        axis=args.axis,
                        trim_ratio=args.trim_ratio,
                        sampling=args.sampling,
                        norm_mode=args.norm_mode,
                        norm_scope=args.norm_scope,
                        center_crop=args.center_crop,
                        clahe=args.clahe,
                    )
                    slice_imgs = [_resize_pil(im, args.resize) for im in slice_imgs]
                    content = []
                    for j, (im, idx) in enumerate(zip(slice_imgs, slice_indices)):
                        if args.slice_markers:
                            content.append({"type": "text", "text": f"Slice {j+1}/{len(slice_imgs)} (index={idx}):"})
                        content.append({"type": "image", "image": im})
                    content.append({"type": "text", "text": prompt})
                    messages = [{"role": "user", "content": content}]

                inputs = processor(
                    conversation=messages,
                    return_tensors="pt",
                    add_system_prompt=True,
                    add_generation_prompt=True,
                )

                # 关键：别静默，让你看到生成是否为0 token
                if "pixel_values" in inputs and isinstance(inputs["pixel_values"], torch.Tensor):
                    logger.info(f"[{sid}] pixel_values.shape={tuple(inputs['pixel_values'].shape)}")
                if "grid_sizes" in inputs and isinstance(inputs["grid_sizes"], torch.Tensor):
                    logger.info(f"[{sid}] grid_sizes={inputs['grid_sizes'].tolist()}")
                if "merge_sizes" in inputs and isinstance(inputs["merge_sizes"], torch.Tensor):
                    logger.info(f"[{sid}] merge_sizes={inputs['merge_sizes'].tolist()}")

                inputs = {k: v.to(model.device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}
                if "pixel_values" in inputs and isinstance(inputs["pixel_values"], torch.Tensor):
                    inputs["pixel_values"] = inputs["pixel_values"].to(torch.bfloat16)

                input_ids = inputs.get("input_ids")
                input_len = int(input_ids.shape[1]) if isinstance(input_ids, torch.Tensor) else 0

                with torch.inference_mode():
                    generation = model.generate(
                        **inputs,
                        max_new_tokens=max(args.max_new_tokens, 64),
                        min_new_tokens=max(args.min_new_tokens, 1),
                        do_sample=False,
                        pad_token_id=tokenizer.eos_token_id,
                    )

                gen_len = int(generation.shape[1])
                logger.info(f"[{sid}] input_len={input_len}, gen_len={gen_len}, new={max(0, gen_len - input_len)}")

                # 如果 new==0，直接把整段 decode（避免你trim后变空）
                if input_len > 0 and gen_len > input_len:
                    gen_trim = generation[0, input_len:]
                else:
                    gen_trim = generation[0]

                decoded = decode_best(processor, tokenizer, gen_trim)
                label, conf, _raw = parse_model_output(decoded)

                if label is None:
                    skipped += 1
                    skip_stats["parse_fail_or_empty"] += 1
                    logger.warning(f"[{sid}] SKIP parse_fail_or_empty. output[:200]={decoded[:200]!r}")
                    with log_path.open("a", encoding="utf-8") as lf:
                        lf.write(json.dumps({
                            "sample_id": sid,
                            "skip_reason": "parse_fail_or_empty",
                            "raw_output": decoded,
                            "nifti_path": nifti_path,
                            "pack_3d": args.pack_3d,
                            "input_len": input_len,
                            "gen_len": gen_len,
                            "elapsed_sec": time.time() - t0,
                        }, ensure_ascii=False) + "\n")
                    continue

                pred_row = {
                    "sample_id": sid,
                    "task": "task1",
                    "labels": [label],
                    "confidences": ({label: float(conf)} if conf is not None else {}),
                }
                out_f.write(json.dumps(pred_row, ensure_ascii=False) + "\n")
                out_f.flush()
                written += 1

                with log_path.open("a", encoding="utf-8") as lf:
                    lf.write(json.dumps({
                        "sample_id": sid,
                        "dataset": sample.get("dataset"),
                        "patient_id": sample.get("patient_id"),
                        "nifti_path": nifti_path,
                        "pack_3d": args.pack_3d,
                        "raw_output": decoded,
                        "final": pred_row,
                        "input_len": input_len,
                        "gen_len": gen_len,
                        "elapsed_sec": time.time() - t0,
                    }, ensure_ascii=False) + "\n")

            except Exception as e:
                skipped += 1
                skip_stats["exception"] += 1
                logger.error(f"[{sid}] EXCEPTION: {e}")
                logger.error(traceback.format_exc())
                with log_path.open("a", encoding="utf-8") as lf:
                    lf.write(json.dumps({
                        "sample_id": sid,
                        "skip_reason": "exception",
                        "error": str(e),
                        "traceback": traceback.format_exc(),
                        "nifti_path": nifti_path,
                        "pack_3d": args.pack_3d,
                        "elapsed_sec": time.time() - t0,
                    }, ensure_ascii=False) + "\n")
                continue

    logger.info(f"Done. written={written}, skipped={skipped}, elapsed_total_sec={time.time() - t0_all:.1f}")
    logger.info(f"Skip stats: {dict(skip_stats)}")


if __name__ == "__main__":
    main()
#!/usr/bin/env python3
"""
Hulu-Med-7B inference (3D MRI NIfTI) - SKIP-FREE / DEBUGGABLE version

核心修复：
1) min_new_tokens 防止 generation==input（0 new tokens）
2) decode 两次：use_think=False 失败则 use_think=True
3) JSON 兼容单引号 (ast.literal_eval) + 更鲁棒的label提取
4) parse 支持中文关键词（心梗/瘢痕/正常/无）
5) 每个 skip 都打印原因与输出片段，最后汇总 skip_reason 统计
"""

import argparse
import json
import re
import time
import logging
import traceback
import ast
from collections import defaultdict
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import numpy as np
from PIL import Image
import torch
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoProcessor

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

import sys
sys.path.insert(0, str(Path(__file__).parent.parent))

from utils.data_utils import load_jsonl, load_nifti, apply_clahe_uint8

MI_LABEL = "myocardial_infarction_or_scar"
NEG_LABEL = "normal_or_no_lesion"
_JSON_OBJ_RE = re.compile(r"\{[\s\S]*\}")


def get_input_path(sample: Dict, preferred_type: str) -> Optional[str]:
    for inp in sample.get("inputs", []) or []:
        if inp.get("type") == preferred_type:
            return inp.get("path")
    return None


def select_uniform_indices(n: int, k: int, trim_ratio: float = 0.05) -> List[int]:
    if n <= 0 or k <= 0:
        return []
    if n <= k:
        return list(range(n))
    lo = int(round(trim_ratio * (n - 1)))
    hi = int(round((1.0 - trim_ratio) * (n - 1)))
    if hi <= lo or (hi - lo + 1) < k:
        lo, hi = 0, n - 1
    xs = np.linspace(lo, hi, num=k)
    idxs = [int(round(x)) for x in xs]
    seen, out = set(), []
    for i in idxs:
        if 0 <= i < n and i not in seen:
            out.append(i)
            seen.add(i)
    return out


def select_center_contiguous_indices(n: int, k: int, trim_ratio: float = 0.05) -> List[int]:
    if n <= 0 or k <= 0:
        return []
    if n <= k:
        return list(range(n))
    lo = int(round(trim_ratio * (n - 1)))
    hi = int(round((1.0 - trim_ratio) * (n - 1)))
    if hi <= lo:
        lo, hi = 0, n - 1
    span = hi - lo + 1
    if span <= 0:
        return []
    if span <= k:
        return list(range(lo, hi + 1))
    mid = (lo + hi) // 2
    start = mid - (k // 2)
    start = max(lo, min(start, hi - k + 1))
    return list(range(start, start + k))


def volume_to_pil_images(
    vol: np.ndarray,
    *,
    num_slices: int,
    axis: str = "z",
    trim_ratio: float = 0.05,
    sampling: str = "center",  # uniform | center
    norm_mode: str = "percentile",  # percentile | foreground
    norm_scope: str = "slice",  # slice | volume
    center_crop: int = 0,
    clahe: bool = False,
) -> Tuple[List[Image.Image], List[int]]:
    if vol.ndim != 3:
        raise ValueError(f"Expected 3D volume, got shape={vol.shape}")

    if axis == "z":
        n = vol.shape[2]
        get_slice = lambda idx: vol[:, :, idx]
    elif axis == "y":
        n = vol.shape[1]
        get_slice = lambda idx: vol[:, idx, :]
    elif axis == "x":
        n = vol.shape[0]
        get_slice = lambda idx: vol[idx, :, :]
    else:
        raise ValueError(f"Unsupported axis: {axis}")

    indices = (
        select_center_contiguous_indices(n, num_slices, trim_ratio=trim_ratio)
        if sampling == "center"
        else select_uniform_indices(n, num_slices, trim_ratio=trim_ratio)
    )

    def compute_lo_hi(arr: np.ndarray) -> Tuple[float, float]:
        arr = np.asarray(arr, dtype=np.float32)
        vals = arr[arr != 0] if norm_mode == "foreground" else arr.reshape(-1)
        if vals.size == 0:
            vals = arr.reshape(-1)
        lo = float(np.percentile(vals, 1))
        hi = float(np.percentile(vals, 99))
        if hi <= lo:
            hi = lo + 1.0
        return lo, hi

    vol_lo = vol_hi = None
    if norm_scope == "volume":
        vol_lo, vol_hi = compute_lo_hi(vol)

    images: List[Image.Image] = []
    for idx in indices:
        sl = get_slice(idx)
        if sl.ndim == 3:
            sl = sl[..., 0]

        lo, hi = (vol_lo, vol_hi) if (norm_scope == "volume" and vol_lo is not None) else compute_lo_hi(sl)
        sl = np.clip(sl, lo, hi)
        sl = (sl - lo) / (hi - lo + 1e-8)
        sl = np.clip(sl, 0.0, 1.0)
        if norm_mode == "foreground":
            sl = np.where(sl > 0, sl, 0.0)

        u8 = (sl * 255.0).astype(np.uint8)

        if center_crop and u8.ndim == 2:
            h, w = u8.shape
            size = min(int(center_crop), h, w)
            if size > 0:
                cx, cy = w // 2, h // 2
                x0 = max(0, cx - size // 2)
                y0 = max(0, cy - size // 2)
                u8 = u8[y0:y0 + size, x0:x0 + size]

        if clahe:
            try:
                u8 = apply_clahe_uint8(u8)
            except Exception:
                pass

        img = Image.fromarray(u8, mode="L").convert("RGB")
        images.append(img)

    return images, indices


def _resize_pil(img: Image.Image, size: int) -> Image.Image:
    if not size or size <= 0:
        return img
    try:
        return img.resize((size, size), Image.Resampling.BILINEAR)
    except AttributeError:
        return img.resize((size, size), Image.BILINEAR)


def build_prompt(num_images: int, style: str) -> str:
    # 强烈建议用 final_answer（最不容易 parse 失败）
    if style == "final_answer":
        return (
            f"Analyze this cardiac MRI volume with {num_images} slices. "
            f"Check specifically for {MI_LABEL}. "
            "Answer with EXACTLY one line: 'Final Answer: yes' or 'Final Answer: no'. "
            "('yes' means MI present; 'no' means normal/no lesion)"
        )
    return (
        f"Analyze this cardiac MRI volume with {num_images} slices. "
        f"Check specifically for {MI_LABEL}. "
        "You must output ONLY a valid JSON object, no other text. "
        f'Format: {{"label": "{MI_LABEL}" or "{NEG_LABEL}", "confidence": 0.0-1.0}}'
    )


def parse_model_output(text: str) -> Tuple[Optional[str], Optional[float], str]:
    raw = (text or "").strip()
    if not raw:
        return None, None, raw
    low = raw.lower()

    # 1) Final Answer
    m = re.search(r"final\s*answer\s*:\s*(yes|no)", low)
    if m:
        return (MI_LABEL if m.group(1) == "yes" else NEG_LABEL), None, raw

    # 2) 直接包含标签字符串
    if MI_LABEL in raw:
        return MI_LABEL, None, raw
    if NEG_LABEL in raw:
        return NEG_LABEL, None, raw

    # 3) JSON / dict 解析（支持单引号）
    m = _JSON_OBJ_RE.search(raw)
    blob = m.group(0) if m else None
    candidates = [blob] if blob else []
    candidates.append(raw)  # 最后整段也试一遍（有的人不输出大括号）

    for cand in candidates:
        if not cand:
            continue
        obj = None
        try:
            obj = json.loads(cand)
        except Exception:
            try:
                obj = ast.literal_eval(cand)  # 支持 {'label': '...'}
            except Exception:
                obj = None
        if isinstance(obj, dict):
            label = obj.get("label") or obj.get("pred") or obj.get("diagnosis")
            conf = obj.get("confidence")
            if isinstance(label, str):
                label = label.strip()
            conf = float(conf) if isinstance(conf, (int, float)) else None
            if label in ("yes", "Yes", "YES", True, "true", "True"):
                return MI_LABEL, conf, raw
            if label in ("no", "No", "NO", False, "false", "False"):
                return NEG_LABEL, conf, raw
            if label in (MI_LABEL, NEG_LABEL):
                return label, conf, raw

    # 4) yes/no 兜底
    if re.search(r"\byes\b", low):
        return MI_LABEL, None, raw
    if re.search(r"\bno\b", low):
        return NEG_LABEL, None, raw

    # 5) 中文关键词兜底
    if ("心梗" in raw) or ("心肌梗死" in raw) or ("瘢痕" in raw) or ("梗死" in raw):
        return MI_LABEL, None, raw
    if ("正常" in raw) or ("未见" in raw) or ("无明显" in raw) or ("未发现" in raw) or ("无梗死" in raw) or ("无瘢痕" in raw):
        return NEG_LABEL, None, raw

    return None, None, raw


def decode_best(processor, tokenizer, token_ids_1d: torch.Tensor) -> str:
    # 先去 think，再保留 think（避免“只有think没有final导致空串”）
    try:
        t = processor.decode(token_ids_1d.tolist(), skip_special_tokens=True, use_think=False).strip()
    except Exception:
        t = ""
    if not t:
        try:
            t = processor.decode(token_ids_1d.tolist(), skip_special_tokens=True, use_think=True).strip()
        except Exception:
            t = tokenizer.decode(token_ids_1d.tolist(), skip_special_tokens=True).strip()

    if "<|im_start|>assistant" in t:
        t = t.split("<|im_start|>assistant")[-1].strip()
    if "<|im_end|>" in t:
        t = t.replace("<|im_end|>", "").strip()
    return t


def main():
    parser = argparse.ArgumentParser(description="Hulu-Med-7B baseline inference (3D NIfTI) - debugged")
    parser.add_argument("--model_path", type=str, default="hf_models/Hulu-Med-7B")
    parser.add_argument("--index_jsonl", type=str, default="benchmark/data/index/index.jsonl")
    parser.add_argument("--datasets", type=str, default="ACDC,EMIDEC,MnMs2")
    parser.add_argument("--sample_ids", type=str, default="outputs/sample_ids_A_mi_binary_all_nifti_n200.json")
    parser.add_argument("--max_samples", type=int, default=1)

    parser.add_argument("--num_slices", type=int, default=8)
    parser.add_argument("--axis", type=str, default="z", choices=["x", "y", "z"])
    parser.add_argument("--trim_ratio", type=float, default=0.05)
    parser.add_argument("--no_canonical", action="store_true")
    parser.add_argument("--sampling", type=str, default="center", choices=["uniform", "center"])
    parser.add_argument("--norm_mode", type=str, default="percentile", choices=["percentile", "foreground"])
    parser.add_argument("--norm_scope", type=str, default="slice", choices=["slice", "volume"])
    parser.add_argument("--center_crop", type=int, default=0)
    parser.add_argument("--slice_markers", action="store_true")
    parser.add_argument("--clahe", action="store_true")

    parser.add_argument("--prompt_style", type=str, default="final_answer", choices=["json", "final_answer"])
    parser.add_argument("--max_new_tokens", type=int, default=256)
    parser.add_argument("--min_new_tokens", type=int, default=32)

    parser.add_argument("--output", type=str, default="outputs/predictions/hulumed_test_single.jsonl")
    parser.add_argument("--log_jsonl", type=str, default="outputs/logs/hulumed_test_single.jsonl")

    # 最稳：multi_image（完全绕开 video/3d RoPE 分支）
    parser.add_argument("--pack_3d", type=str, default="multi_image", choices=["multi_image", "native_3d"])
    parser.add_argument("--resize", type=int, default=280)

    args = parser.parse_args()

    model_path = Path(args.model_path)
    if not model_path.exists():
        raise FileNotFoundError(f"model_path not found: {model_path}")

    torch.backends.cuda.matmul.allow_tf32 = True
    torch.set_float32_matmul_precision("high")

    logger.info(f"Loading Hulu-Med from: {model_path}")
    model = AutoModelForCausalLM.from_pretrained(
        str(model_path),
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True,
        local_files_only=True,
    )
    processor = AutoProcessor.from_pretrained(
        str(model_path),
        trust_remote_code=True,
        local_files_only=True,
    )
    tokenizer = processor.tokenizer
    model.eval()

    all_samples = load_jsonl(args.index_jsonl)
    if args.sample_ids:
        target_ids = set(json.loads(Path(args.sample_ids).read_text(encoding="utf-8")))
        samples = [s for s in all_samples if s.get("sample_id") in target_ids]
    else:
        samples = all_samples

    if args.datasets:
        ds = set(x.strip() for x in args.datasets.split(",") if x.strip())
        samples = [s for s in samples if s.get("dataset") in ds]

    samples = [s for s in samples if s.get("task1")]
    if args.max_samples is not None:
        samples = samples[: args.max_samples]

    out_path = Path(args.output)
    out_path.parent.mkdir(parents=True, exist_ok=True)
    log_path = Path(args.log_jsonl)
    log_path.parent.mkdir(parents=True, exist_ok=True)
    out_path.write_text("", encoding="utf-8")

    logger.info(f"Found {len(samples)} samples. pack_3d={args.pack_3d}, prompt_style={args.prompt_style}")

    skipped = 0
    written = 0
    skip_stats = defaultdict(int)
    t0_all = time.time()

    with out_path.open("a", encoding="utf-8") as out_f:
        for sample in tqdm(samples, desc="Hulu-Med推理中"):
            sid = sample.get("sample_id")
            nifti_path = get_input_path(sample, "nifti")
            t0 = time.time()

            try:
                if not sid or not nifti_path or not Path(nifti_path).exists():
                    raise FileNotFoundError(f"missing_nifti: sid={sid}, path={nifti_path}")

                # 读取volume（multi_image必需；native_3d也先读一下便于排查）
                vol = load_nifti(nifti_path, canonical=(not args.no_canonical))
                if vol.ndim == 4:
                    vol = vol[..., 0]
                if vol.ndim != 3:
                    raise ValueError(f"Expected 3D volume after squeeze, got {vol.shape}")

                prompt = build_prompt(args.num_slices, args.prompt_style)

                axis_map = {"z": 2, "y": 1, "x": 0}
                nii_axis = axis_map.get(args.axis, 2)

                if args.pack_3d == "native_3d":
                    messages = [{
                        "role": "user",
                        "content": [
                            {"type": "3d", "3d": {"image_path": nifti_path, "nii_num_slices": args.num_slices, "nii_axis": nii_axis}},
                            {"type": "text", "text": prompt},
                        ],
                    }]
                else:
                    # multi_image：最稳
                    slice_imgs, slice_indices = volume_to_pil_images(
                        vol,
                        num_slices=args.num_slices,
                        axis=args.axis,
                        trim_ratio=args.trim_ratio,
                        sampling=args.sampling,
                        norm_mode=args.norm_mode,
                        norm_scope=args.norm_scope,
                        center_crop=args.center_crop,
                        clahe=args.clahe,
                    )
                    slice_imgs = [_resize_pil(im, args.resize) for im in slice_imgs]
                    content = []
                    for j, (im, idx) in enumerate(zip(slice_imgs, slice_indices)):
                        if args.slice_markers:
                            content.append({"type": "text", "text": f"Slice {j+1}/{len(slice_imgs)} (index={idx}):"})
                        content.append({"type": "image", "image": im})
                    content.append({"type": "text", "text": prompt})
                    messages = [{"role": "user", "content": content}]

                inputs = processor(
                    conversation=messages,
                    return_tensors="pt",
                    add_system_prompt=True,
                    add_generation_prompt=True,
                )

                # 关键：别静默，让你看到生成是否为0 token
                if "pixel_values" in inputs and isinstance(inputs["pixel_values"], torch.Tensor):
                    logger.info(f"[{sid}] pixel_values.shape={tuple(inputs['pixel_values'].shape)}")
                if "grid_sizes" in inputs and isinstance(inputs["grid_sizes"], torch.Tensor):
                    logger.info(f"[{sid}] grid_sizes={inputs['grid_sizes'].tolist()}")
                if "merge_sizes" in inputs and isinstance(inputs["merge_sizes"], torch.Tensor):
                    logger.info(f"[{sid}] merge_sizes={inputs['merge_sizes'].tolist()}")

                inputs = {k: v.to(model.device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}
                if "pixel_values" in inputs and isinstance(inputs["pixel_values"], torch.Tensor):
                    inputs["pixel_values"] = inputs["pixel_values"].to(torch.bfloat16)

                input_ids = inputs.get("input_ids")
                input_len = int(input_ids.shape[1]) if isinstance(input_ids, torch.Tensor) else 0

                with torch.inference_mode():
                    generation = model.generate(
                        **inputs,
                        max_new_tokens=max(args.max_new_tokens, 64),
                        min_new_tokens=max(args.min_new_tokens, 1),
                        do_sample=False,
                        pad_token_id=tokenizer.eos_token_id,
                    )

                gen_len = int(generation.shape[1])
                logger.info(f"[{sid}] input_len={input_len}, gen_len={gen_len}, new={max(0, gen_len - input_len)}")

                # 如果 new==0，直接把整段 decode（避免你trim后变空）
                if input_len > 0 and gen_len > input_len:
                    gen_trim = generation[0, input_len:]
                else:
                    gen_trim = generation[0]

                decoded = decode_best(processor, tokenizer, gen_trim)
                label, conf, _raw = parse_model_output(decoded)

                if label is None:
                    skipped += 1
                    skip_stats["parse_fail_or_empty"] += 1
                    logger.warning(f"[{sid}] SKIP parse_fail_or_empty. output[:200]={decoded[:200]!r}")
                    with log_path.open("a", encoding="utf-8") as lf:
                        lf.write(json.dumps({
                            "sample_id": sid,
                            "skip_reason": "parse_fail_or_empty",
                            "raw_output": decoded,
                            "nifti_path": nifti_path,
                            "pack_3d": args.pack_3d,
                            "input_len": input_len,
                            "gen_len": gen_len,
                            "elapsed_sec": time.time() - t0,
                        }, ensure_ascii=False) + "\n")
                    continue

                pred_row = {
                    "sample_id": sid,
                    "task": "task1",
                    "labels": [label],
                    "confidences": ({label: float(conf)} if conf is not None else {}),
                }
                out_f.write(json.dumps(pred_row, ensure_ascii=False) + "\n")
                out_f.flush()
                written += 1

                with log_path.open("a", encoding="utf-8") as lf:
                    lf.write(json.dumps({
                        "sample_id": sid,
                        "dataset": sample.get("dataset"),
                        "patient_id": sample.get("patient_id"),
                        "nifti_path": nifti_path,
                        "pack_3d": args.pack_3d,
                        "raw_output": decoded,
                        "final": pred_row,
                        "input_len": input_len,
                        "gen_len": gen_len,
                        "elapsed_sec": time.time() - t0,
                    }, ensure_ascii=False) + "\n")

            except Exception as e:
                skipped += 1
                skip_stats["exception"] += 1
                logger.error(f"[{sid}] EXCEPTION: {e}")
                logger.error(traceback.format_exc())
                with log_path.open("a", encoding="utf-8") as lf:
                    lf.write(json.dumps({
                        "sample_id": sid,
                        "skip_reason": "exception",
                        "error": str(e),
                        "traceback": traceback.format_exc(),
                        "nifti_path": nifti_path,
                        "pack_3d": args.pack_3d,
                        "elapsed_sec": time.time() - t0,
                    }, ensure_ascii=False) + "\n")
                continue

    logger.info(f"Done. written={written}, skipped={skipped}, elapsed_total_sec={time.time() - t0_all:.1f}")
    logger.info(f"Skip stats: {dict(skip_stats)}")


if __name__ == "__main__":
    main()
