#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Convert the per-turn Numina (RFF-G) parquet (generated by examples/data_preprocess/numina.py)
into an AgentLoop-friendly *multi-turn* parquet for `step_verify_agent`.

Input (from numina.py)
----------------------
Each row corresponds to ONE forward_agent turn, with fields like:
- data_source: str
- prompt: List[{"role","content"}]  (system + user(backward hint))
- response: str                    (reference forward_agent output for that turn)
- reward_model: {"style":"rule","ground_truth": ...}
- extra_info: dict including:
    - turn_index: int
    - problem_id: str
    - problem_text: str
    - backward_hint: str
    - interaction_kwargs: dict (optional; used by other pipelines)

Output (for StepVerifyAgentLoop)
-------------------------------
Each row corresponds to ONE problem/conversation, with fields like:
- data_source: str
- agent_name: "step_verify_agent"
- prompt: List[{"role","content"}]  (system + user for the FIRST subproblem)
- ability: str
- reward_model: dict (copied from input; can be used by your reward fn if desired)
- extra_info:
    - index: int
    - problem_id / problem_text / goals / goal_order / final_goal_id ... (if available)
    - subproblems: List[str]        # user messages per step, in order
    - expected_answers: List[str]   # optional: reference forward outputs per step (for rule verifier)

Then `StepVerifyAgentLoop` will:
1) start from `prompt`
2) after each assistant generation, verify step reward
3) append next user subproblem from `extra_info.subproblems`
4) produce token-level rm_scores via spans + turn_scores (mean spread).
"""

from __future__ import annotations

import argparse
import os
from collections import defaultdict
from typing import Any, Dict, List, Tuple

import datasets


def _safe_get_user_text(prompt: Any) -> str:
    """Extract the last user content from a messages list."""
    if not isinstance(prompt, list):
        return ""
    for m in reversed(prompt):
        if isinstance(m, dict) and m.get("role") == "user":
            return str(m.get("content", ""))
    return ""


def _safe_get_system_text(prompt: Any) -> str:
    if not isinstance(prompt, list):
        return ""
    for m in prompt:
        if isinstance(m, dict) and m.get("role") == "system":
            return str(m.get("content", ""))
    return ""


def _sort_key(row: Dict[str, Any]) -> Tuple[int, int]:
    ei = row.get("extra_info") or {}
    # turn_index is preferred; fall back to 0
    turn_index = int(ei.get("turn_index", 0) or 0)
    # stable tie-breaker: keep original order if available
    # (datasets does not preserve original row id reliably; we use 0 here)
    return turn_index, 0


def _group_rows(rows: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
    grouped: Dict[str, List[Dict[str, Any]]] = defaultdict(list)
    for r in rows:
        ei = r.get("extra_info") or {}
        pid = str(ei.get("problem_id") or "")
        if not pid:
            continue
        grouped[pid].append(r)
    return grouped


def build_multiturn_sample(problem_rows: List[Dict[str, Any]], index: int, include_expected_answers: bool) -> Dict[str, Any]:
    # Sort turns
    problem_rows = sorted(problem_rows, key=_sort_key)
    first = problem_rows[0]

    data_source = first.get("data_source", "rffg-math-numina-data")
    ability = first.get("ability", "math")
    reward_model = first.get("reward_model", {"style": "rule", "ground_truth": None})

    # subproblems: we use each row's user message (which already includes backward hint and possibly the problem text)
    subproblems: List[str] = []
    expected_answers: List[str] = []

    for r in problem_rows:
        user_text = _safe_get_user_text(r.get("prompt"))
        if not user_text:
            # fallback: use backward_hint if prompt is missing
            ei = r.get("extra_info") or {}
            user_text = str(ei.get("backward_hint") or "")
        subproblems.append(user_text)

        if include_expected_answers:
            expected_answers.append(str(r.get("response", "") or ""))

    # prompt: system + first user
    system_text = _safe_get_system_text(first.get("prompt"))
    if not system_text:
        # If missing, keep a minimal system prompt; you can override via config.chat_template later
        system_text = "You are a helpful assistant."
    prompt = [
        {"role": "system", "content": system_text},
        {"role": "user", "content": subproblems[0] if subproblems else ""},
    ]

    # Carry over useful metadata (best-effort)
    first_ei = first.get("extra_info") or {}
    interaction_kwargs = first_ei.get("interaction_kwargs") or {}
    extra_info: Dict[str, Any] = {
        "index": index,
        "problem_id": first_ei.get("problem_id"),
        "problem_text": first_ei.get("problem_text"),
        "final_goal_id": interaction_kwargs.get("final_goal_id"),
        "goals": interaction_kwargs.get("goals"),
        "goal_order": interaction_kwargs.get("goal_order"),
        "subproblems": subproblems,
    }
    if include_expected_answers:
        extra_info["expected_answers"] = expected_answers

    # Also keep any fields you might need downstream
    # (e.g., to provide context to your verifier function)
    extra_info["num_steps"] = len(subproblems)

    return {
        "data_source": data_source,
        "agent_name": "step_verify_agent",
        "prompt": prompt,
        "ability": ability,
        "reward_model": reward_model,
        "extra_info": extra_info,
    }


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--in_parquet", required=True, help="Input parquet produced by numina.py (per-turn rows).")
    ap.add_argument("--out_parquet", required=True, help="Output parquet (per-problem multi-turn rows).")
    ap.add_argument(
        "--include_expected_answers",
        action="store_true",
        help="If set, write extra_info.expected_answers from per-turn `response` (for rule verifier).",
    )
    ap.add_argument(
        "--emit",
        choices=["per_problem", "prefix"],
        default="per_problem",
        help=(
            "Output granularity. "
            "`per_problem` emits one row per problem_id (so row count becomes #unique problems). "
            "`prefix` emits multiple rows per problem_id, one for each prefix length k (1..num_steps), "
            "which keeps multi-turn format but increases dataset size."
        ),
    )
    ap.add_argument(
        "--min_prefix_len",
        type=int,
        default=1,
        help="Only used when --emit=prefix. Minimum prefix length k to emit (default: 1).",
    )
    ap.add_argument(
        "--max_prefix_len",
        type=int,
        default=0,
        help="Only used when --emit=prefix. If >0, cap prefix length to this value.",
    )
    args = ap.parse_args()

    in_path = os.path.abspath(os.path.expanduser(args.in_parquet))
    out_path = os.path.abspath(os.path.expanduser(args.out_parquet))
    os.makedirs(os.path.dirname(out_path), exist_ok=True)

    ds = datasets.load_dataset("parquet", data_files=in_path, split="train")
    rows = [ds[i] for i in range(len(ds))]

    grouped = _group_rows(rows)
    problem_ids = sorted(grouped.keys())

    out_rows: List[Dict[str, Any]] = []

    # Stats to explain "data size drop"
    input_rows = len(rows)
    rows_with_pid = sum(1 for r in rows if str((r.get("extra_info") or {}).get("problem_id") or "").strip())
    num_problems = len(problem_ids)
    num_steps_total = sum(len(grouped[pid]) for pid in problem_ids)
    avg_steps = (num_steps_total / num_problems) if num_problems else 0.0
    print(
        f"[STATS] input_rows={input_rows}, rows_with_problem_id={rows_with_pid}, "
        f"unique_problem_ids={num_problems}, avg_steps_per_problem={avg_steps:.2f}"
    )

    out_index = 0
    for pid in problem_ids:
        prs = grouped[pid]
        if not prs:
            continue
        if args.emit == "per_problem":
            out_rows.append(
                build_multiturn_sample(prs, index=out_index, include_expected_answers=args.include_expected_answers)
            )
            out_index += 1
        else:
            # prefix mode: emit one sample per prefix length k
            base = build_multiturn_sample(prs, index=out_index, include_expected_answers=args.include_expected_answers)
            subproblems = (base.get("extra_info") or {}).get("subproblems") or []
            expected_answers = (base.get("extra_info") or {}).get("expected_answers") or []
            total_k = len(subproblems)
            if args.max_prefix_len and args.max_prefix_len > 0:
                total_k = min(total_k, args.max_prefix_len)
            k0 = max(1, int(args.min_prefix_len))
            for k in range(k0, total_k + 1):
                sample = dict(base)
                sample_extra = dict(sample.get("extra_info") or {})
                sample_extra["subproblems"] = list(subproblems[:k])
                if args.include_expected_answers:
                    sample_extra["expected_answers"] = list(expected_answers[:k])
                sample_extra["prefix_len"] = k
                sample["extra_info"] = sample_extra
                sample_extra["index"] = out_index
                out_rows.append(sample)
                out_index += 1

    if not out_rows:
        raise RuntimeError(f"No output rows produced from {in_path}. Check input schema and problem_id fields.")

    out_ds = datasets.Dataset.from_list(out_rows)
    out_ds.to_parquet(out_path)
    print(f"[OK] output_rows={len(out_rows)}, saved -> {out_path}")


if __name__ == "__main__":
    main()


